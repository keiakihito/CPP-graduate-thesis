Section 3.1 Graph Walk

1. What is a "Graph Walk" for Synthetic Data?
A Graph Walk is a probabilistic method used to generate sequences of data. In your case, it simulates a user listening to a playlist. Instead of picking songs randomly (which would look like noise to BERT4Rec), you treat your music archive as a connected network (a graph).
• The Nodes: Every song in the iPalpiti archive is a "node."
• The Edges: Connections between songs. If Song A is acoustically similar to Song B (similar Tempo, Genre, or Embedding), there is a strong "edge" connecting them.
• The Walk: To create a synthetic user session, you start at a random node (Song A) and "walk" to a neighbor (Song B) based on the strength of the connection.
Why is this necessary? Your proposal uses BERT4Rec. As noted in Abbattista et al., sequential models like BERT4Rec are designed to capture the "dynamic nature of music consumption" and create a "coherent listening experience"
. If you feed BERT4Rec random data, it cannot learn the "sequential and passive consumption behaviors" typical of music listeners
. A Graph Walk ensures that your synthetic data has logical transitions (e.g., adagio → adagio, or violin → orchestra), giving the model patterns to learn.

--------------------------------------------------------------------------------
2. How to Describe it in Your Thesis (Drafting Guide)
In your methodology section (likely Chapter 3), you should frame this not just as "generating data," but as "Simulating Realistic Sequential Consumption Patterns."
Here is a structured way to write this section, referencing your specific sources:
A. Justification (Why you didn't use random data)
You must explain that random selection fails to model real user behavior.
• Drafting Text: "Music consumption is inherently sequential and repetitive
. Listeners rarely select tracks in a purely stochastic manner; rather, their choices are influenced by the acoustic and stylistic properties of the previously consumed track
. To train the BERT4Rec sequential model effectively in the absence of real user logs, it was necessary to generate synthetic sessions that mimic these coherent transition patterns. Randomly sampling tracks would result in a noise distribution that prevents the Transformer architecture from learning meaningful temporal dependencies."
B. Graph Construction (How you built the map)
Explain that you built a similarity graph using the features from Shi and Tamm.
• Drafting Text: "We constructed a directed similarity graph G=(V,E), where V represents the set of tracks in the iPalpiti archive. The edge weights Eij​ between track i and track j were calculated using the Cosine Similarity of their audio embeddings. Following the feature extraction methodology of Shi (2025), we ensured the graph topology reflected stylistic and emotional continuity by utilizing Chroma vectors (harmonic content) and Short-Time RMS (energy/dynamics) alongside the deep embeddings
."
C. The Random Walk Algorithm (The Transition Logic)
Describe the actual "Walk." This solves the "Synthetic Sequence Challenge" mentioned in the polishing steps
.
• Drafting Text: "Synthetic listening sessions were generated by performing probabilistic random walks on this similarity graph. Given a current track ti​, the probability of transitioning to a next track tj​ was proportional to their acoustic similarity score. This ensures that the synthetic users exhibit 'Transition Logic,' moving between tracks with compatible tempos and timbres, thereby providing the sequential signals required for BERT4Rec optimization
."
D. Incorporating "Repeated Consumption" (The Loop)
Abbattista emphasizes that users repeat songs. A standard graph walk might never return to the start. You must add a probability to "loop back."
• Drafting Text: "Furthermore, Abbattista et al. (2024) identify 'repeated consumption' as a critical signal in music recommendation, noting that transformer models often underperform when they fail to account for users returning to favorite tracks
. To model this 'Personalized Popularity'
, our graph walk algorithm includes a variable restart probability, allowing the synthetic session to periodically return to high-affinity nodes (previously listened tracks), ensuring the mock data reflects the repetitive nature of real-world music consumption."

--------------------------------------------------------------------------------
3. Implementation Logic for Sprint 3.1
To implement this in Sprint 3.1, you do not need complex graph database software. You can do this with simple matrix math in Python (NumPy/Pandas):
1. Calculate Similarity Matrix: Compute the Cosine Similarity between every song and every other song. This is your "Adjacency Matrix."
2. Define Probabilities: For a given row (Song A), normalize the similarity scores so they sum to 1.0. These are your transition probabilities.
3. The Walk Loop:
    ◦ current_song = Random Start.
    ◦ Append current_song to session_list.
    ◦ Logic Check: Roll a die. If random < 0.2 (Repeated Consumption factor
), pick a song already in session_list.
    ◦ Else: Pick next_song using numpy.random.choice based on the transition probabilities calculated in Step 2.
    ◦ Repeat until session length is reached (e.g., 10-20 songs).
By describing it this way, you satisfy the academic requirement of citing Abbattista's behavioral theories
 and Shi's feature requirements, while solving the practical engineering problem of training BERT4Rec.