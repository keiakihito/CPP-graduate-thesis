\chapter{Literature Review} \label{ch:LiteratureReview}

The domain of music recommendation has historically been dominated by Collaborative Filtering (CF) approaches, which rely on dense user-interaction matrices. However, specialized archives such as the \textbf{iPalpiti Music Archive} typically lack the massive volume of user data required for these methods, leading to the ``Cold Start'' problem. This literature review synthesizes thirteen key research contributions that collectively argue for a \textbf{Deep Content-Based} approach. By leveraging pretrained audio representations, sequential modeling, and efficient neural architectures, it is possible to build high-performance recommendation systems that rely on the audio signal itself rather than historical usage data.

\section{Pretrained Audio Representations and Feature Extraction}
The core technical foundation of this thesis rests on the ability to extract meaningful semantic features directly from raw audio waveforms or spectrograms.

Tamm et al. (2024) \cite{tamm2024} provide the primary methodological framework for this research. Their comparative analysis of pretrained models---including \textbf{MusicNN}, \textbf{MERT}, and \textbf{Jukebox}---demonstrates that ``frozen'' embeddings from these models can effectively drive recommendation tasks. Crucially for this project, they highlight that lighter, discriminative models often yield a better trade-off between computational cost and accuracy compared to massive generative models, validating the choice of efficient backends for an AWS Lambda-based architecture.

Pons and Serra (2019) \cite{pons2019} introduced \textbf{MusicNN}, a cornerstone model in this domain. They demonstrated that music-specific Convolutional Neural Networks (CNNs) could outperform standard generic audio models by using distinct filter shapes (timbral vs. temporal). This project utilizes MusicNN as a primary candidate for the feature extraction pipeline due to its proven efficiency in tagging and representation learning.

Expanding on the potential of transformer-based architectures for audio, Ramos et al. \cite{ramos2024} explored \textbf{Self-Supervised Learning (SSL)} using the \textbf{Audio Spectrogram Transformer (AST)} within a SimCLR framework. Training on the Free Music Archive (FMA) dataset with InfoNCE contrastive loss, they demonstrated that music embeddings can be learned without explicit labels, organizing tracks by composition, timbre, and flow rather than conventional genre classifications. Notably, their qualitative evaluation showed that the trained model achieved 48\% satisfactory recommendations compared to only 10\% for the untrained baseline, with the learned representations capturing ``subtle elements of musical structure'' beyond obvious metadata. This supports our thesis that unsupervised embeddings can uncover the latent ``musicality'' of the iPalpiti archive without manual annotation.

Further emphasizing efficiency, Maleki et al. \cite{maleki2024} proposed the \textbf{Compact Convolutional Transformer (CCT)}. Their work addresses the ``Continuity Problem'' in streaming, showing that a hybrid CNN-Transformer model with only \textasciitilde450k parameters (specifically 454,187) can achieve state-of-the-art genre classification with 93.75\% test accuracy on the GTZAN dataset, outperforming previous CRNN-based models while maintaining significantly lower computational complexity. This aligns with Reddy et al.'s \cite{reddy2024} research on \textbf{MusicNet}, a compact CNN for real-time background music detection optimized for edge deployment. MusicNet achieves 81.3\% TPR at 0.1\% FPR while being only 0.2 MB in size---10x smaller than competing models---with 11.1ms inference time (4x faster than best-performing alternatives). Crucially, MusicNet incorporates \textbf{in-model featurization}, processing raw audio directly without requiring external feature extraction, simplifying deployment and maintenance in production systems. Both papers contribute to the project's goal of \textbf{cost-efficiency and scalability} (RQ3), proving that high-performance audio analysis does not require prohibitive computational resources.

\section{Sequential User Modeling and Recommendation Logic}
While audio features describe \textit{what} a track sounds like, recommendation logic must understand \textit{how} users consume music over time.

Sun et al. (2019) \cite{sun2019} introduced \textbf{BERT4Rec}, applying the Bidirectional Encoder Representations from Transformers (BERT) architecture to sequential recommendation. Unlike unidirectional models (SASRec), BERT4Rec uses Cloze tasks (masking items) to learn bidirectional context. This project adopts BERT4Rec to model ``listening sessions'' generated from mock data, predicting the next track based on the acoustic flow of the session rather than just static similarity.

However, Abbattista et al. \cite{abbattista2024} offer a critical counter-perspective. Their study on \textbf{Personalized Popularity Awareness} revealed that complex transformer models often underperform compared to simple baselines because they fail to account for ``repeated consumption'' (users re-listening to favorites). While the iPalpiti archive focuses on discovery, this insight suggests that the recommendation engine should perhaps include a ``Personalized Most Popular'' signal or a mechanism to handle repeat listening, preventing the model from over-optimizing for novelty.

Lin et al. \cite{lin2024} propose a \textbf{hybrid architecture} that combines a Deep CNN for audio emotion modeling with a Self-Attention mechanism for user modeling. Although this project scopes out explicit emotion recognition, Lin's architectural pattern---fusing a static content vector (Audio CNN) with a dynamic context vector (Self-Attention)---directly informs our ``Hybrid Strategy'' (RQ2), where we combine frozen embeddings with sequential user state.

Schedl et al. \cite{schedl2024} further refine user modeling by identifying \textbf{Country Archetypes} based on geographic listening behavior and unsupervised clustering. Using t-SNE and OPTICS on 369 million listening events from 70 countries, they identified 9 distinct country clusters reflecting shared music preferences at the track level. Their ``geo-aware'' VAE architecture extends standard collaborative filtering by incorporating geographic context through a gating mechanism, testing four user models (country ID, cluster ID, cluster distances, country distances). Results demonstrated that all context-aware models significantly outperformed baseline VAE, with improvements of 3.4-7.4\% across precision, recall, and NDCG metrics. For our project, this contributes to the design of the \textbf{Mock Data Generation} phase (Methodology Phase 1), suggesting that synthetic users should be modeled not just randomly, but as distinct ``listener archetypes'' (e.g., ``Fast Tempo Violin Enthusiast'' vs. ``Orchestral Purist'') to train the backend models effectively.

\section{Deep Content-Based and Hybrid Recommendation Systems}
Several papers validate the end-to-end viability of using CNN-based audio features for recommendation.

Dias et al. \cite{dias2024} developed a system using CNNs for genre classification and recommendation, explicitly addressing the metadata bottleneck that arises from manual genre labeling. Their CNN-based approach achieved 76\% accuracy on the GTZAN dataset, demonstrating the viability of deep learning for music genre recognition. This supports the decision to use CNN-based audio feature extraction as the frontend for the recommendation system.

Zhang et al. \cite{zhang2024} similarly proposed a CNN-based system that constructs user preference vectors by aggregating the classification features of their listening history. Using MFCC and mel spectrogram features extracted from 100 digital piano pieces across four genres (classical, pop, rock, pure music), they compared two user modeling approaches: \textbf{``Comprehensive''} (single averaged feature vector, achieving 50.35\% accuracy) vs. \textbf{``Multicategory''} (distinct category-specific vectors, achieving higher accuracy than single-category). This comparison offers a concrete strategy for the \textbf{Shallow Network} tier of our methodology, specifically in how we map a user's mock history to a single point in the embedding space.

Prasad et al. \cite{prasad2024} provide a high-level technical briefing on \textbf{AI-Powered Recommendation}, emphasizing the shift from metadata-based to AI-driven systems to solve the ``Cold Start'' problem. Their system architecture integrates multiple ML approaches: supervised learning (decision trees, random forests, neural networks) for classification, unsupervised learning (K-Means, DBSCAN) for discovering hidden patterns, and reinforcement learning (Deep Q-learning, Multi-Armed Bandit) for continuous improvement through user feedback. Achieving 94\% recommendation accuracy with 92\% prediction accuracy for user preferences, their discussion of ``In-Model Featurization'' and the separation of concerns between data collection, feature extraction, and recommendation generation parallels our architectural decision to decouple feature extraction (Lambda/Fargate) from the recommendation serving layer.

\section{Evaluation and User Impact}
Finally, the ultimate goal of recommendation is to influence user experience positively.

Porcaro et al. \cite{porcaro2024} conducted a \textbf{12-week longitudinal study with 110 participants} on the \textbf{Impact of Diversity} in music recommendations. Focusing on Electronic Music exposure, they found that high-diversity recommendations significantly increased users' openness to unfamiliar genres, fueled curiosity, and helped deconstruct genre stereotypes. Specifically, they measured both implicit attitudes (via Single Category IAT) and explicit openness (via Guttman scale), demonstrating that exposure diversity positively impacts listeners' willingness to explore new music. This is particularly relevant for the \textbf{iPalpiti Music Archive}, whose mission is to expose listeners to specialized, potentially unfamiliar classical performances. It suggests that our evaluation metrics (Evaluation Phase 3) should look beyond simple accuracy (HitRate) and consider \textbf{Diversity} or \textbf{Novelty} metrics to ensure the system is effectively surfacing the ``long tail'' of the archive.
