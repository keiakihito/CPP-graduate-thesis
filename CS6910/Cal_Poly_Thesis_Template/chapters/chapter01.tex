\chapter{Introduction} \label{ch:Introduction}

The digital preservation of music archives presents a unique challenge: making content discoverable without the massive user interaction data that powers commercial giants like Spotify or Apple Music. The iPalpiti Music Archive, featuring recordings by award-winning musicians from the iPalpiti Festival, is a prime example of such a specialized domain. In these contexts, traditional collaborative filtering fails due to data sparsity.

Recent advancements in Music Information Retrieval (MIR) have shown that deep learning models can extract rich semantic information directly from audio signals. The work of Tamm et al. \cite{tamm2024} demonstrates that "frozen" embeddings from pretrained models (e.g., MusicNN, MERT) can serve as powerful features for recommendation systems, even with limited data.

This thesis proposes to adapt these findings to the iPalpiti archive, building a hybrid recommendation engine on AWS. By leveraging serverless infrastructure (Lambda, S3, RDS), the system aims to provide real-time, content-aware recommendations that help users navigate the emotional and stylistic landscape of the archive.

\section{Research Goal} \label{sec:ResearchGoal}

\subsection{Problem Statement}
Small-scale music institutions possess valuable cultural assets but lack the technical resources to build personalized discovery tools. Existing state-of-the-art models are often evaluated on generic pop datasets (e.g., MTG-Jamendo) and require expensive infrastructure.

\subsection{Objective}
This research aims to:
\begin{enumerate}
    \item Adapt the Tamm et al. methodology to the specific domain of the iPalpiti archive.
    \item Compare the performance of different audio backends (MusicNN vs. MERT) in this classical/performance-focused domain.
    \item Implement a production-ready, serverless recommendation API on AWS that demonstrates cost-efficiency and scalability.
\end{enumerate}

\subsection{Research Questions}
\begin{itemize}
    \item \textbf{RQ1}: How do pretrained audio representations perform in recommendation tasks when trained and evaluated on small-scale or domain-specific datasets (e.g., classical music)?
    \item \textbf{RQ2}: To what extent do different pretrained models overfit or generalize when the dataset size is reduced?
    \item \textbf{RQ3}: How does dataset size influence the relative performance ranking among pretrained audio models?
    \item \textbf{RQ4 (Optional)}: Which aspects of audio embeddings (genre, timbre, dynamics) remain robust or degrade under small-sample fine-tuning or transfer?
\end{itemize}
