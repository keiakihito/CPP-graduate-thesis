Technical Report: A Comparative Performance Analysis of Pretrained Backend Models for Music Recommender Systems

1.0 Introduction and Objective

In the development of content-aware Music Recommender Systems (MRS), a strategic gap exists between two key communities. The Music Information Retrieval (MIR) community has produced a variety of powerful audio models pretrained on vast music datasets, proving their effectiveness in downstream tasks like genre classification and auto-tagging. Conversely, the Recommender Systems (RS) community has historically favored traditional end-to-end learning, often overlooking the potential of these pretrained representations. Bridging this gap offers significant advantages, including the ability to address the persistent cold-start problem for new items and the opportunity to leverage large, unlabeled music datasets, which are far more accessible than proprietary datasets containing both audio and user listening histories.

The primary objective of this report is to synthesize and evaluate the performance of six distinct pretrained audio backend models within the specific context of Music Recommender Systems. This analysis aims to provide a clear performance hierarchy and practical guidance for engineers and data scientists on which audio representations are most effective for predicting user preferences.

To achieve this, our investigation is guided by three core research questions:

* RQ1: Are pretrained audio representations a viable option for MRS?
* RQ2: How do different backend models compare in the context of MRS?
* RQ3: How does pretrained backend model performance in MRS correspond to performance in MIR tasks?

To rigorously test these questions, we now outline the experimental methodology, covering the models, dataset, and evaluation framework used in our analysis.

2.0 Experimental Methodology

A robust and transparent methodology is essential for a credible evaluation of model performance. This section outlines the three core components of our experimental design: the pretrained audio representations that form the basis of our content features, the recommendation model architectures used to test their effectiveness, and the dataset and evaluation protocol that ensure a fair and consistent comparison.

2.1 Pretrained Audio Backend Models

This analysis evaluates six prominent pretrained backend models against a traditional Mel Frequency Cepstral Coefficients (MFCCs) baseline. Each model produces audio embeddings with distinct characteristics derived from its unique architecture and training paradigm. The core characteristics of each backend model are summarized in the table below.

Model	Brief Description	Embedding Size
MFCC	A low-level acoustic feature designed to capture the timbral characteristics of an audio signal.	104
MusiCNN	Supervised CNN trained on predicting 50 crowd-sourced music tags.	200
MusicFM	Self-supervised Conformer-based model using random projection tokenization.	750
EncodecMAE	Masked Autoencoder applied to neural audio codec representations.	768
Music2Vec	Self-supervised Transformer model trained via masked prediction of teacher outputs.	768
MERT	Masked language model using acoustic and musical teacher models for pseudo-labels.	1024
Jukebox	Music generation model using Vector Quantized Variational Autoencoders (VQ-VAE).	4800

All of these models produce embeddings for short audio chunks. To create a single, unified representation for each music track, we average these chunk-level embeddings over time.

2.2 Recommendation System Architectures

To assess the utility of the audio embeddings, we integrated them into three recommendation models of increasing complexity.

1. K-Nearest Neighbours (KNN): A simple, content-only approach where a user's representation is calculated by averaging the embeddings of all items in their listening history. Recommendations are then generated by finding the items closest to this average user vector. This method allows for a direct assessment of the raw information contained within the embeddings.
2. Shallow Net: A shallow neural network that combines content and collaborative signals. User and item IDs are passed through embedding layers. The item embedding layer is initialized with the pretrained audio embeddings, and its weights are frozen to isolate the information content of the pretrained representations. The user embedding layer is initialized with a mean of the user’s tracks, but its weights are unfrozen and can be updated during training. The score for a user-item pair is the cosine similarity between their resulting vectors. The model is trained using a Max Margin Hinge Loss function.
3. BERT4Rec: A powerful sequential recommendation model based on the bidirectional Transformer architecture. To incorporate content information, the pretrained embeddings were integrated as a frozen projection of the model's item embedding module. Due to memory constraints, the input sequence length was limited to 300 items, which may impact performance for users with very long listening histories.

To provide a crucial reference point, we also evaluated a "random initialization" variant for the Shallow Net and BERT4Rec models. This pure collaborative filtering approach learns item embeddings from scratch, serving as a baseline to determine whether the frozen pretrained embeddings provide a genuine performance boost or act as an unnecessary constraint.

2.3 Dataset and Evaluation Metrics

The experiment was conducted using the Music4All-Onion dataset, which provides user listening histories, music audio samples, and precomputed MFCC features.

The dataset was split chronologically. The last month of listening history (after February 20, 2020) was used for testing and validation, while the preceding year was used for training. To ensure a fair evaluation of known items, users and items that did not appear in the training data (i.e., cold users and items) were removed from the validation and test sets. The final dataset splits are detailed below.

	Train	Validation	Test
Num Users	17,053	6,092	6,092
Num Items	56,193	36,942	37,797
Num Interactions	5,122,221	132,425	138,299

Model performance was assessed using the following primary evaluation metrics. For all evaluations, tracks that a user had already listened to were removed from the recommendation lists.

* HitRate@50
* Recall@50
* NDCG@50 (Normalized Discounted Cumulative Gain)

Having established the experimental framework, we now present the performance outcomes generated from this setup.

3.0 Comparative Performance Results

This section presents the core quantitative findings of the study. The performance of each pretrained audio embedding is analyzed across the three distinct recommendation architectures—KNN, Shallow Net, and BERT4Rec—to identify consistent trends and establish a clear performance hierarchy.

3.1 Analysis of Raw Embedding Performance (KNN)

The K-Nearest Neighbours (KNN) model provides a direct measure of the utility of the raw, unprocessed audio embeddings for recommendation. The performance ranking based on HitRate@50 and NDCG@50 is as follows:

1. MusiCNN: The clear top performer, demonstrating significantly higher effectiveness than all other models.
2. Jukebox
3. EncodecMAE
4. MERT
5. Music2Vec
6. MFCC (Baseline)
7. MusicFM: The worst-performing model, scoring well below even the MFCC baseline.

3.2 Analysis of Hybrid Model Performance (Shallow Net)

When integrated into the Shallow Net architecture, the integration with collaborative data yielded a crucial insight: the combination of content and collaborative signals is not merely additive, but multiplicative in its effect. All backend models dramatically outperform both their pure KNN variants and the pure collaborative "Random" initialization variant. Specifically, all backend models enriched with collaborative information show 10 times better performance than their respective raw KNN variants. This result clearly shows the synergy between content embeddings and collaborative data, where each component enhances the other to produce a far more effective model.

The performance ranking within this hybrid model is:

1. MusiCNN: Again, the top performer by a clear margin.
2. EncodecMAE
3. MERT & Music2Vec (tied)
4. Jukebox
5. MFCC (Baseline)
6. MusicFM

3.3 Analysis of Sequential Model Performance (BERT4Rec)

The integration with the complex, sequential BERT4Rec model provides the most nuanced results. Not all embeddings were able to enhance the already powerful collaborative baseline.

* MusiCNN was the only model to provide a statistically significant improvement over the pure collaborative BERT4Rec baseline.
* MERT and EncodecMAE performed comparably to the baseline, indicating that they neither significantly helped nor hindered the model.
* MusicFM, Music2Vec, and Jukebox actively degraded the performance of the pure collaborative model. The significant performance drop for Jukebox may be attributable to its extremely large embedding dimension (4800), which can be challenging for models to effectively utilize without more elaborate processing.

3.4 Overall Performance Synthesis

Synthesizing the results across all three recommendation architectures reveals a consistent performance hierarchy. The following table summarizes the overall ranking and key characteristics of each model's performance.

Rank	Model	Performance Summary
1	MusiCNN	Consistently the top-performing model across all three recommendation architectures, from the simplest KNN to the complex BERT4Rec.
2	MERT & EncodecMAE	Delivered strong and comparable results, performing well in hybrid settings and matching the strong collaborative baseline in BERT4Rec.
3	Music2Vec	Exhibited moderate performance, generally outperforming the MFCC baseline but falling short of the top-tier models and hindering BERT4Rec.
4	Jukebox & MusicFM	Showed variable but generally poor performance. Jukebox performed well with KNN but its effectiveness dropped with increasing model complexity. MusicFM consistently underperformed across all tests.

Having established a clear performance hierarchy for music recommendation, the analysis will now explore how these results compare to the models' documented performance in traditional Music Information Retrieval tasks.

4.0 Discrepancies between MRS and MIR Task Performance

A key strategic question for practitioners is whether a model's excellence in technical MIR tasks—such as genre classification, key detection, or auto-tagging—translates to high performance in the more nuanced task of predicting user preference. This analysis reveals that strong performance in one domain does not guarantee success in the other.

The table below compares self-reported model performance on several standard MIR benchmarks with the recommendation performance observed in this study.

Table 1: Performance Comparison: MIR Tasks vs. Music Recommender Systems The 'Tags' column represents AUC on MagnaTagATune; 'Genres' is accuracy on GTZAN; 'Key' is accuracy on Giantsteps. The 'Recs' column represents HitRate@50 with BERT4Rec from this study.

Model	Tags	Genres	Key	Recs
MusicFM	0.924	—	0.674	0.261
Music2Vec	0.895	0.766	0.508	0.281
MERT	0.913	0.793	0.656	0.360
EncodecMAE	—	0.862	—	0.349
Jukebox	0.915	0.797	0.667	0.219
MusiCNN	0.906	0.790	0.128	0.385

Analyzing the key discrepancies in this table reveals several important insights:

* There is a stark contrast in performance for MusicFM and Jukebox. These models are top performers in auto-tagging and key prediction, respectively, but were among the worst-performing models for music recommendation.
* MusiCNN exhibits the inverse profile. It delivers superior recommendation performance despite having lower scores in some MIR tasks, most notably a very low score in key detection.
* MERT and EncodecMAE show relatively strong and balanced performance across both MIR and MRS tasks, suggesting their learned representations capture information that is broadly valuable for both technical classification and user preference modeling.

This divergence strongly suggests that the aspects of musical information most valuable for user-preference-driven Music Recommender Systems differ from those prioritized for traditional MIR tasks. This has significant implications for model selection, and the following section distills these findings into actionable recommendations.

5.0 Key Findings and Practical Recommendations

This report set out to answer three core research questions regarding the use of pretrained audio models in Music Recommender Systems. This section provides direct answers to those questions and offers practical guidance for data scientists and ML engineers building these systems.

5.1 RQ1: Are pretrained audio representations a viable option for MRS?

Yes, absolutely. The experiments demonstrate that incorporating pretrained audio representations is a viable and highly effective method for improving pure collaborative models. This is particularly valuable as it can be achieved by using the embeddings as frozen initializations, enhancing performance without the need for computationally expensive model finetuning or end-to-end retraining.

5.2 RQ2: How do different backend models compare in the context of MRS?

There is a clear performance hierarchy among the tested models for MRS applications.

* Top Recommendations: MusiCNN, MERT, and EncodecMAE are the top choices. MusiCNN consistently delivered the best results across all tested architectures. The fact that it was trained on a supervised auto-tagging task suggests that its representations capture semantic information (related to genres, instruments, and emotions) that is highly relevant to user preferences.
* Use with Caution: Practitioners should be cautious when considering MusicFM and Jukebox. MusicFM showed inferior performance in all MRS tests despite its strong MIR benchmarks. Jukebox performed well in the simple KNN model, but its effectiveness degraded significantly in more complex architectures, possibly due to its very large embedding dimension.

5.3 RQ3: How does pretrained backend model performance in MRS correspond to performance in MIR tasks?

Strong performance on MIR benchmarks does not directly translate to strong performance in MRS. Models like MusicFM and Jukebox, which excel at technical tasks like auto-tagging and key prediction, performed poorly in recommendation. Conversely, MusiCNN, the best model for recommendations, was not the top performer on several MIR tasks. This indicates that the optimal audio representation is task-dependent; the features that best predict genre are not necessarily the same ones that best predict user affinity.

Having addressed the core research questions, we now acknowledge the study's scope before summarizing the key takeaways.

6.0 Limitations and Future Work

For scientific rigor and to guide future research, it is important to acknowledge the limitations of the current analysis. The findings of this report are robust within their defined scope, but several factors warrant further investigation.

The primary limitations of this study include:

* Single Dataset: The analysis relied exclusively on the Music4All-Onion dataset, which may limit the generalizability of the findings to other populations or music catalogs.
* Limited Recommendation Models: While the study used three distinct architectures, a broader exploration of different recommendation models could reveal further nuances in how embeddings perform.
* Single Integration Method: We explored only one method for incorporating embeddings: using them as frozen initializations for an item embedding layer. Alternative approaches, such as using content embeddings to predict or regularize collaborative embeddings, remain unexplored.
* No End-to-End Comparison: The study did not include a comparison to end-to-end trained CNN models, which are a common benchmark in content-aware MRS.
* No Cold-Start Evaluation: The performance of these models in a true cold-start scenario, a key potential benefit of this approach, was not evaluated.

These limitations represent clear and valuable directions for future research in this domain, which could further refine our understanding of how to best leverage pretrained audio models in recommender systems.

7.0 Conclusion

This report provides a comprehensive comparison of six pretrained audio backend models for the task of music recommendation. Our findings confirm that incorporating these representations is a viable and effective strategy for enhancing the performance of pure collaborative models. The analysis establishes a clear performance hierarchy, identifying MusiCNN, MERT, and EncodecMAE as the most effective models for this application. A crucial takeaway is that the utility of a model is highly task-dependent; top performance on traditional MIR tasks does not guarantee strong performance in MRS. In particular, the supervised auto-tagging task used to train MusiCNN appears to generate representations that are exceptionally well-suited for predicting user preferences. We hope this work proves helpful in inspiring the broader adoption of pretrained audio representations in the design and implementation of next-generation Music Recommender Systems.
