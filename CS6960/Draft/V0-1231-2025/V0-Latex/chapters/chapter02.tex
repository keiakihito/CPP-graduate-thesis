\chapter{Literature Review}

The domain of music recommendation has historically been dominated by Collaborative Filtering (CF) approaches, which rely on dense user-interaction matrices. However, specialized archives such as the iPalpiti Music Archive typically lack the massive volume of user data required for these methods, leading to the "Cold Start" problem. This literature review synthesizes twelve key research contributions that collectively argue for a Deep Content-Based approach. By leveraging pretrained audio representations, sequential modeling, and efficient neural architectures, it is possible to build high-performance recommendation systems that rely on the audio signal itself rather than historical usage data.

\section{Pretrained Audio Representations and Feature Extraction}
The core technical foundation of this thesis rests on the ability to extract meaningful semantic features directly from raw audio waveforms or spectrograms. Tamm et al. (2024) \cite{tamm2024} provide the primary methodological framework for this research. Their comparative analysis of six pretrained models---MusiCNN, MERT, Jukebox, MusicFM, Music2Vec, and EncodecMAE---demonstrates that "frozen" embeddings from these models can effectively drive recommendation tasks. Crucially, MusiCNN (a lighter, supervised discriminative model trained on auto-tagging) achieved the best performance across all recommendation methods (HitRate@50: 0.385), significantly outperforming larger generative models like Jukebox (4800-dimensional embeddings). This validates the choice of efficient, discriminative backends for an AWS Lambda-based architecture.

\subsection{Compact Architectures for Resource-Constrained Deployment}
Pourmoazemi and Maleki (2024) \cite{pourmoazemi2024} address the "Continuity Problem" in music streaming by proposing a Compact Convolutional Transformer (CCT) architecture for genre-based recommendation. Their hybrid model combines six convolutional layers (filters: 32, 64, 128) for local feature extraction from mel-spectrograms, followed by two transformer encoders with multi-head attention (2 heads, 128-dimensional). The CCT achieves 93.75\% test accuracy on the GTZAN dataset while containing only 454,187 parameters---significantly fewer than state-of-the-art CRNN models. For recommendation, they use cosine similarity between feature maps, demonstrating that learned representations effectively capture genre-specific patterns for content-based music retrieval.

\subsection{Transfer Learning and Cross-Validation Strategies}
Lin et al. (2024) \cite{lin2024prototype} propose a Transfer Learning + CNN + GRU (TL-CNN-GRU) model that leverages pretrained MobileNetV2 weights for spatial feature extraction combined with bidirectional GRU (1024 units) for capturing temporal dependencies. Their architecture applies 10-Fold Cross-Validation (10-FCV) to mitigate overfitting. The model achieved 71\% accuracy on GTZAN, representing significant improvement over TL+CNN baseline (53\%) and TL+CNN+GRU (55\%). With 7,779,402 total parameters (5,521,418 trainable, 2,257,984 frozen), the system demonstrates that transfer learning from vision models pretrained on ImageNet can be repurposed for audio classification. Genre-specific F1-scores improved substantially with 10-FCV: blues ($0.49 \rightarrow 0.74$), classical ($0.87 \rightarrow 0.92$), metal ($0.61 \rightarrow 0.84$), and reggae ($0.35 \rightarrow 0.69$), though rock remained challenging ($0.26 \rightarrow 0.50$).

\subsection{Self-Supervised Learning for Unlabeled Data}
Ramos et al. \cite{ramos2023self} explored Self-Supervised Learning (SSL) using the Audio Spectrogram Transformer (AST) within a SimCLR framework. Training on the Free Music Archive (FMA) dataset with InfoNCE contrastive loss, they demonstrated that music embeddings can be learned without explicit labels, organizing tracks by composition, timbre, and flow rather than conventional genre classifications. Their qualitative evaluation showed that the trained model achieved 48\% satisfactory recommendations compared to only 10\% for the untrained baseline, with learned representations capturing "subtle elements of musical structure" beyond obvious metadata.

\subsection{Edge Deployment and In-Model Featurization}
Reddy et al. \cite{reddy2022} developed MusicNet, a compact CNN for real-time background music detection optimized for edge deployment. MusicNet achieves 81.3\% TPR at 0.1\% FPR while being only 0.2 MB in size---10x smaller than competing models---with 11.1ms inference time (4x faster than best-performing alternatives). MusicNet incorporates in-model featurization, processing raw audio directly without requiring external feature extraction, simplifying deployment and maintenance in production systems. Together with Pourmoazemi and Maleki's CCT, these papers contribute to the project's goal of cost-efficiency and scalability (RQ3).

\section{Sequential User Modeling and Recommendation Logic}
While audio features describe what a track sounds like, recommendation logic must understand how users consume music over time. This section examines strategies for modeling temporal listening patterns and user preferences.

\subsection{Personalized Popularity Awareness}
Abbattista et al. \cite{abbattista2024} offer a critical counter-perspective. Their study on Personalized Popularity Awareness revealed that complex transformer models often underperform compared to simple baselines because they fail to account for "repeated consumption" (users re-listening to favorites). While the iPalpiti archive focuses on discovery, this insight suggests that the recommendation engine should perhaps include a "Personalized Most Popular" signal or a mechanism to handle repeat listening, preventing the model from over-optimizing for novelty.

\subsection{Hybrid Content and Sequential Modeling}
Lin et al. \cite{lin2025deep} propose a hybrid architecture that combines a Deep CNN for audio emotion modeling with a Self-Attention mechanism for user emotion modeling. Their system integrates three components: (1) Deep Convolutional Neural Network (DCNN) for extracting emotional features from audio signals using spectrograms, (2) Self-Attention Mechanism (specifically Scaled Dot-Product Attention with Multi-Head Attention) for capturing temporal dynamics in user emotional states, and (3) collaborative filtering enhanced with Neural Collaborative Filtering (NCF) and SVD++. Achieving 82\% emotion matching accuracy and 83\% recommendation accuracy (Precision@10), their hybrid approach significantly outperforms traditional content-based filtering (63\% emotion matching, 62\% Precision@10) and collaborative filtering (66\% emotion matching, 68\% Precision@10). Although this project scopes out explicit emotion recognition, Lin's architectural pattern---fusing a static content vector (Audio CNN) with a dynamic context vector (Self-Attention)---directly informs our "Hybrid Strategy" (RQ2).

\subsection{User Archetypes and Clustering-Based Modeling}
Schedl et al. \cite{schedl2021} further refine user modeling by identifying Country Archetypes based on geographic listening behavior and unsupervised clustering. Using t-SNE and OPTICS on 369 million listening events from 70 countries, they identified 9 distinct country clusters reflecting shared music preferences at the track level. Their "geo-aware" VAE architecture extends standard collaborative filtering by incorporating geographic context through a gating mechanism, testing four user models (country ID, cluster ID, cluster distances, country distances). Results demonstrated that all context-aware models significantly outperformed baseline VAE, with relative improvements of 4.9-7.4\% across precision, recall, and NDCG metrics. For our project, this contributes to the design of the Mock Data Generation phase (Methodology Phase 1), suggesting that synthetic users should be modeled not just randomly, but as distinct "listener archetypes" (e.g., "Fast Tempo Violin Enthusiast" vs. "Orchestral Purist").

\section{End-to-End System Architectures and Deployment Strategies}
Beyond theoretical model design, practical recommendation systems require careful architectural decisions regarding feature aggregation, user representation, and deployment infrastructure. The following papers demonstrate strategies for operationalizing deep learning models in production environments.

\subsection{User Preference Aggregation via Feature Averaging}
Zhang \cite{zhang2022} proposes a CNN-based system that constructs user preference vectors by aggregating the classification features of their listening history. Using MFCC and mel spectrogram features extracted from 400 digital piano pieces (100 per genre) across four genres (classical, pop, rock, pure music), they compared two user modeling approaches: "Comprehensive" (single averaged feature vector, achieving 50.35\% accuracy) vs. "Multicategory" (distinct category-specific vectors, achieving 42.89\% accuracy overall but performing better for multicategory users). The Comprehensive approach achieved higher overall accuracy, while the Multicategory approach was more effective for users with diverse genre preferences. This comparison directly informs the Shallow Network tier of our methodology (Methodology Phase 2), specifically demonstrating how to map a user's listening history to a single point in the embedding space through feature averaging---a computationally efficient approach suitable for serverless deployment.

\subsection{API Integration and Microservice Deployment}
Lin et al. \cite{lin2024prototype} demonstrate practical deployment integration by connecting their TL-CNN-GRU model to external music platforms. Their prototype system integrates the trained model with YouTube and Spotify APIs to provide real-time genre-based recommendations, bridging the gap between offline model training and online serving. This validates that deep learning recommendation models can be deployed as microservices that interface with existing music streaming infrastructure, supporting our AWS Lambda-based architecture where feature extraction and recommendation logic exist as separate, scalable services.

\subsection{Modular Architecture and Separation of Concerns}
Prasad et al. \cite{prasad2023} provide a comprehensive architectural framework for AI-Powered Recommendation Systems, emphasizing modular design principles. Their architecture separates concerns between data collection, feature extraction, and recommendation generation, integrating multiple ML paradigms: supervised learning (decision trees, random forests, neural networks) for classification, unsupervised learning (K-Means, DBSCAN) for pattern discovery, and reinforcement learning (Deep Q-learning, Multi-Armed Bandit) for continuous improvement. This modular approach directly parallels our architectural decision to decouple feature extraction (Lambda/Fargate for audio processing) from the recommendation serving layer, enabling independent scaling and maintenance of each component.

\subsection{Addressing the Metadata Bottleneck}
Dias et al. \cite{dias2022} further validate the end-to-end viability of CNN-based genre classification for recommendation, achieving 76\% accuracy on the GTZAN dataset. By explicitly addressing the metadata bottleneck that arises from manual genre labeling, their work reinforces the core thesis premise: deep learning can replace manual annotation pipelines, making content-based recommendation viable even for archives lacking comprehensive metadata.

\section{Beyond Accuracy: Diversity and Long-Tail Discovery}
Traditional recommendation evaluation focuses on accuracy metrics like HitRate or Precision@K, which measure how often the system correctly predicts user preferences. However, for specialized archives whose mission is educational and exploratory, diversity and novelty become equally important success criteria.

Porcaro et al. \cite{porcaro2022} conducted a 12-week longitudinal study with 110 participants on the Impact of Diversity in music recommendations. Focusing on Electronic Music exposure, they found that high-diversity recommendations significantly increased users' openness to unfamiliar genres, fueled curiosity, and helped deconstruct genre stereotypes. Specifically, they measured both implicit attitudes (via Single Category IAT) and explicit openness (via Guttman scale), demonstrating that exposure diversity positively impacts listeners' willingness to explore new music. This is particularly relevant for the iPalpiti Music Archive, whose mission is to expose listeners to specialized, potentially unfamiliar classical performances. It suggests that our evaluation metrics (Evaluation Phase 3) should look beyond simple accuracy (HitRate) and consider Diversity or Novelty metrics to ensure the system is effectively surfacing the "long tail" of the archive.

\section{Synthesis and Contributions to the Thesis}
These twelve papers collectively provide the theoretical foundation, technical methodologies, and evaluation frameworks necessary to address the four research questions guiding this thesis on small-scale, domain-specific music recommendation.

\textbf{Addressing RQ1 (Small-Scale Dataset Performance)}: Papers \cite{tamm2024, ramos2023self, reddy2022, abbattista2024, lin2025deep} demonstrate that pretrained audio representations can perform effectively even with limited training data. Tamm et al. \cite{tamm2024} establish that frozen embeddings from models like MusiCNN achieve HitRate@50 of 0.385, validating that audio signals alone can drive recommendations without requiring massive datasets. Critically, Pourmoazemi and Maleki \cite{pourmoazemi2024} and Lin et al. \cite{lin2024prototype} both evaluate on GTZANâ€”a relatively small dataset (1000 tracks, 100 per genre)---achieving 93.75\% and 71\% accuracy respectively, demonstrating viability for domain-specific archives. Ramos et al. \cite{ramos2023self} show that self-supervised learning on the Free Music Archive can capture "subtle elements of musical structure" beyond obvious metadata, suggesting pretrained models generalize well to specialized domains like classical music. Reddy et al. \cite{reddy2022} prove that compact models (0.2 MB) can achieve production-grade performance, addressing resource constraints typical of small-scale archives.

\textbf{Addressing RQ2 (Overfitting and Generalization)}: Lin et al. \cite{lin2024prototype} directly address overfitting through 10-Fold Cross-Validation (10-FCV), showing that careful validation strategies improve generalization even when dataset size is limited. Their genre-specific F1-scores improved substantially with 10-FCV: blues ($0.49 \rightarrow 0.74$), classical ($0.87 \rightarrow 0.92$), metal ($0.61 \rightarrow 0.84$), reggae ($0.35 \rightarrow 0.69$). Tamm et al. \cite{tamm2024} demonstrate that using frozen embeddings (without fine-tuning) prevents overfitting on small datasets by leveraging knowledge learned from massive pretraining corpora. This approach is particularly relevant for RQ2, as it suggests that transfer learning with frozen representations may generalize better than fine-tuned models when data is scarce. Ramos et al. \cite{ramos2023self} validate self-supervised learning as another strategy for improving generalization without labeled data.

\textbf{Addressing RQ3 (Dataset Size Impact on Model Ranking)}: Tamm et al. \cite{tamm2024} provide crucial evidence that model ranking changes based on dataset characteristics: MusiCNN (a lighter, supervised discriminative model with fewer parameters) outperformed larger generative models like Jukebox (4800-dimensional embeddings), achieving the best HitRate@50 across all recommendation methods. This finding directly supports RQ3 by demonstrating that larger models do not automatically perform better on domain-specific tasks. Pourmoazemi and Maleki \cite{pourmoazemi2024} reinforce this with their Compact Convolutional Transformer (454,187 parameters) achieving 93.75\% accuracy while being "significantly fewer than state-of-the-art CRNN models." These results suggest that when dataset size is limited, compact models may be more efficient and equally effective, challenging the assumption that larger models always rank higher.

\textbf{Addressing RQ4 (Robustness of Embedding Aspects)}: Ramos et al. \cite{ramos2023self} provide qualitative evidence that embeddings capture composition, timbre, and flow even without explicit training on these attributes, suggesting these aspects remain robust under self-supervised learning. Zhang \cite{zhang2022} demonstrates that MFCC and mel spectrogram features extracted from classical music (400 digital piano pieces) retain discriminative power for genre classification (50.35\% accuracy with Comprehensive approach), indicating that spectral and timbral features are robust to small-sample conditions. Lin et al. \cite{lin2025deep} show that emotional features can be extracted from audio spectrograms with 82\% emotion matching accuracy, suggesting that high-level semantic attributes remain accessible even with limited data. However, Lin et al. \cite{lin2024prototype} reveal that genre-specific performance varies: classical music achieved high F1-scores (0.92) while rock struggled (0.50), suggesting that some musical genres may be more challenging to represent with embeddings under small-sample conditions.

\textbf{Supporting Methodologies}: Papers \cite{schedl2021, dias2022, zhang2022} inform the mock data generation strategy (Methodology Phase 1) by demonstrating that user archetypes (Schedl et al. \cite{schedl2021}: country clusters with 4.9-7.4\% performance gains) and popularity-aware modeling (Abbattista et al. \cite{abbattista2024}) improve recommendation quality beyond pure content-based approaches. Papers \cite{reddy2022, prasad2023, dias2022} validate the architectural decisions for AWS serverless deployment, with Zhang \cite{zhang2022} demonstrating efficient user preference aggregation (Comprehensive approach), Lin et al. \cite{lin2024prototype} showing API integration patterns (YouTube/Spotify), and Prasad et al. \cite{prasad2023} providing modular design principles. Porcaro et al. \cite{porcaro2022} establish that diversity metrics are critical for evaluating specialized archives, shaping the evaluation framework (Methodology Phase 3).
