Interspeech 2022
18-22 September 2022, Incheon, Korea

MusicNet: Compact Convolutional Neural Network for Real-time Background Music Detection Chandan K.A. Reddy, Vishak Gopal, Harishchandra Dubey, Sergiy Matusevych, Ross Cutler, Robert Aichner Microsoft Corporation, Redmond, WA, USA chandan.ka@outlook.com, {vigopal,hadubey,sergiym,rcutler,raichner}@microsoft.com

Abstract

development of deep neural networks for audio event detection and classification [13, 14].
We use Audio Set to train a binary classifier for robust music detection. We require a high true positive rate at a very low FPR in the presence of speech, singing, background noise, and in other challenging audio contexts. Specifically we require an FPR ≤ 0.1% so that users don’t receive a false-positive more often that once every 3 hours of continuous evaluation in real-time audio calls, which we’ve determined sufficient for our application in a mainstream audio communication system. In addition,
we require a TPR ≥ 80% to make music detection useful. Our application also requires fast real-time inference on edge devices, a small model size, and low deployment and maintenance costs of the model. Our model contains in-model featurization1 where a 1-D convolutional layer learns to produce spectral features from raw audio.
We validate the model on a realistic test set consisting of a blind test set from the 3rd Deep Noise Suppression Challenge [15, 16, 17, 18] and a collection of real-world music clips created via crowd-sourcing.
The main contributions of MusicNet are (1) It provides a best-in-class TPR evaluated at a low FPR needed for realworld usage, (2) We provide the most extensive evaluation of state-of-the-art music detectors we are aware of, (3) MusicNet has the lowest complexity and smallest model size of any high performing music detectors we’ve evaluated, (4) MusicNet includes in-model featurization for easy integration and maintenance into applications; no source code changes are needed to update model if the input features are changed.

With the recent growth of remote work, online meetings often encounter challenging audio contexts such as background noise, music, and echo. Accurate real-time detection of music events can help to improve the user experience. In this paper, we present MusicNet, a compact neural model for detecting background music in the real-time communications pipeline.
In video meetings, music frequently co-occurs with speech and background noises, making the accurate classification quite challenging. We propose a compact convolutional neural network core preceded by an in-model featurization layer. MusicNet takes 9 seconds of raw audio as input and does not require any model-specific featurization in the product stack. We train our model on the balanced subset of the Audio Set [1] data and validate it on 1000 crowd-sourced real test clips. Finally, we compare MusicNet performance with 20 state-of-the-art models. MusicNet has a true positive rate (TPR) of 81.3% at a 0.1% false-positive rate (FPR), which is significantly better than state-of-the-art models included in our study. MusicNet is also 10x smaller and has 4x faster inference than the best-performing models we benchmarked.
Index Terms: Background Music Detection, Acoustic Event Detection, Instrumental Music, Convolutional Neural Networks, In-Model Featurization.

1. Introduction Audio event detection is not a new challenge for machine perception. The term Audio Event Detection refers to the recognition of sound events in a variety of scenarios. Recently, researchers have proposed several methods for audio event detection in different contexts, e.g., using weak labels, targeting specific audio events, or in challenging signal-to-noise ratio conditions [2, 3, 4]. Detection of specific audio events can also be of interest. Tasks such as speech or voice activity detection for recognizing the presence of human speech [5, 6], or music activity detection [7, 8, 9, 10], have been extensively studied. This paper proposes a solution for low-complexity real-time music detection in the presence of speech and background noises.
Past work in the field typically involved rule-based systems,
used relatively small datasets and a reduced sets of labels, and assumed simpler event-specific settings for event detection and classification [11, 12].
Google Audio Set [1] is a collection of labels associated with 10-second fragments of YouTube videos. Each video fragment is associated with one or more labels from the ontology of 632 sound categories, and the classification is weakly supervised. Audio Set consists of more than two million unique clips, making it the largest publicly available corpus of labeled sounds. The public release of the Audio Set data encouraged the

Copyright © 2022 ISCA

2. Related works The Music Information Retrieval Evaluation eX- change
(MIREX) 2018 challenge [19] fueled the interest of the research community in music detection. This challenge was motivated by the need for speech and/or music detection for real-world tasks such as audio broadcasts where music can co-exists with speech and noise or may occur alone. Detecting audio segments with music can help apply music-specific methods on audio signals to achieve the desired impact [19]. The MIREX challenge has four tasks: Music Detection, Speech Detection, Music & Speech Detection, and Music Relative Loudness Estimation [19]. A CNN model with Mel-scale kernel was leveraged for music detection in broadcasting audio [20]. The Mel-scale kernel in CNN layers helped in learning robust features where the Mel-scale dictates the kernel size. This model was trained on 52 hours of mixed broadcast data containing approximately 50% music; 24 hours of broadcast audio with a music ratio 501 Featurization is the preprocessing of raw audio into features for the DNN classifier

4162

10.21437/Interspeech.2022-864

76% was used as a test set. This test set included representative genres for broadcast audio in English, Spanish, and Korean languages [20].
A recent study [9] conducted a comparative assessment of different convolutional, recurrent, and hybrid deep neural network architecture for music detection trained on mel-spectral features on Audio Set balanced training set [1]. Further, music detection using one neural network and two neural networks were compared. In the second case, one neural network was trained to detect speech and another one for detecting music [9]. The study included fully connected, convolutional, and long short-term memory (LSTM) networks where the hybrid convolutional-LSTM model lead to the best performance [9].
One of the systems [21] submitted at the MIREX 2018 [19]
leveraged a CNN with Mel-scale kernels for music detection and a bidirectional GRU model trained on Mel-spectrograms for speech detection. Another CNN system [22] was submitted at MIREX 2018 for detecting music and estimating the music relative loudness. As music relative loudness represents the loudness corresponding to music content at each frame, thresholding the estimated loudness resulted in music/no-music decision [22].
A novel approach was recently developed for synthesizing the training datasets for a Convolutional Recurrent Neural Network (CRNN) model for speech and music detection which outperformed state-of-the-art approaches [23]. This work provided an effective method for synthesizing a training dataset for speech/music classification. Real-time speech/music classification was proposed using line spectral frequencies and zero-crossing-based features extracted from audio frames [24].
Chroma features, which represent tonality in an audio signal,
were applied for the task of speech/music discrimination [25].
Experiments show that chroma features outperform state-ofthe-art features for speech/music classification on multiple corpora. Experiments on a broadcast news corpus validated the efficacy of chroma features for high-precision music detection even in mismatched acoustic conditions [25].
Novelty in MusicNet comes from three sources: (i) compact architecture which is simple yet effective neural model for background noise for speech enhancement scenarios; (ii) Collecting/synthesizing realistic test set relevant for video/audio calls in Microsoft teams; (iii) Benchmarking the performance of models with respect to 18 state-of-the-art models. The current state-of-the art models are significantly larger in size with much higher inference time. MusicNet takes small memory footprint and does accurate real-time background music detection.

audio to 16kHz as our application is targeted for wideband audio2 . MusicNet is trained to classify 9 second audio clips into music or no-music binary classification. Note that the 9 second window results in an average latency of 4.5 seconds to act on detected music, which is sufficiently low for use in a realtime communication systems, e.g., the user can be prompted to switch to a speech enhancement mode that lets music pass through the noise suppression, or to use an audio codec that is optimized for music quality.
3.2. Test set We collected real-world test clips containing music with or without clean and/or background noise. We also added clips with only clean speech, noisy speech and/or background noises in the test set as no-music examples. We chose the blind test set containing real recordings with music in the background from the 3rd Deep Noise Suppression Challenge [15, 16, 17, 18]. We also included music clips from the publicly available Freesound dataset [26]. Our test set consisted of 1000 real test clips where each clip has approximately a 10 second duration. We added test clips from different instruments such as Piano, Guitar, Violin, Drums, Saxophone, Flute, Cello, Clarinet, Trumpet, Harp,
etc. and different background music genres namely Rock, Pop,
Country, R&B and Soul, Hip Hop/Rap, Electronic, Jazz, Blues,
Classical & Opera, Folk, etc. We synthesized some scenarios for three popular instruments (from music lessons) Piano,
Guitar, and Violin: (i) 20 clips for each instrument type containing only instrumental music. (ii) 10 test clips for each instrument type containing clean speech mixed with instrumental music. In addition, we had three signal to music ratios (SMRs)
to account for three different gains for the musical instruments.
In this way, we get 270 test clips (3 instruments * 30 clips
* 3 SMRs). To this, we added 300 test clips without music where 150 test clips had only clean speech and 150 test clips had noisy speech. Noisy speech and clean speech clips were chosen from the 3rd Deep Noise Suppression Challenge as described in [15, 16, 17, 18]. Two expert listeners independently labeled the test set to ensure that the test set is strongly labeled for music/no-music classification. Our test set consists of music with clean speech, music with noisy speech, music with noise,
only clean speech, only noisy speech, and only noise. The test set was collected through crowd sourcing using a variety of recording devices in both headset and speaker mode. It contains different musical instruments, background music, English speech, and non-English speech with or without background music. Our test set is designed to represent real-world scenarios for video/calls meetings.
Note that the annual event detection challenge DCASE
(https://dcase.community) has N=1378 clips for their test set, but this is spread over 10 event types. Our test set of N=1000 clips for just 1 event type (music) gives a much higher number of clips per event type

3. Proposed Approach 3.1. Training Dataset In this work, we leveraged Google’s Audio Set dataset [1] to train MusicNet. This dataset contains 527 audio events including music. Each clip comes with labels containing names of audio events contained in it. The data is highly skewed in terms of speech and music classes. Both speech and music appear in almost a million clips while the rest of the audio events occur with 100 to 1000 clips for each class. Fortunately, roughly 50%
of the Audio Set clips had music as one of the labels which gives us enough data to train MusicNet. During training, audio clips with music as one of the labels were treated as music while the rest of the clips were treated as non-music. MusicNet was trained on a clip-level which means the loss is computed across all audio clips in a given training batch. We downsampled all

3.3. State-of-the-art Models We compared the performance of MusicNet with 20 state-ofthe-art models described in [27]. To reduce the experimental efforts and make our results reproducible, we leveraged the pretrained models made available on Github [28]. These state-ofthe-art models referred to as PANNs are a set of convolutional neural networks (CNNs) with different complexities taking raw audio waveform or a variety of spectral features as input and 2 https://en.wikipedia.org/wiki/Widebandaudio

4163

Table 1: MusicNet architecture Layer Featurization Conv layer Conv: 32, (3 x 3), ‘ReLU’
MaxPool: (2 x 2), Dropout(0.3)
Conv: 32, (3 x 3), ‘ReLU’
MaxPool: (2 x 2), Dropout(0.3)
Conv: 32, (3 x 3), ‘ReLU’
MaxPool: (2 x 2), Dropout(0.3)
Conv: 64, (3 x 3), ‘ReLU’
GlobalMaxPool Dense: 64, ‘ReLU’
Dense: 64, ‘ReLU’
Dense: 1

to be trained on raw audio and make inference on raw audio, this eliminating the need for implementation of featurization into product stack.
We experimented by adding batch normalization layers after every Conv layer in Table 1. However, adding batch normalization worsens the overall classification accuracy and adversely affected the detection of some musical instruments. We explored other network architectures among which MusicNet as shown in Table 1 generalized the best with the least computational complexity.

Output dimension 144 000 x 1 900 x 120 x 32 450 x 60 x 32 450 x 60 x 32 225 x 30 x 32 225 x 30 x 32 112 x 15 x 32 112 x 15 x 64 1 x 64 1 x 64 1 x 64 1x2

3.5. Real-time Communication Pipeline in Production We deploy the trained model to a wide variety of edge devices,
ranging from Windows, Mac, and Linux PCs to mobile phones
(Android/iOS). For portability, we convert our model into the ONNX format 3 so it can leverage the ONNX Runtime C++
library for client-side inference. Fast real-time inference and low CPU usage are critical requirements for production, and the model size is important when deploying MusicNet to mobile devices over unreliable and/or metered networks. MusicNet is a small model with fast inference and accurate classification and hence is well-suited for our application.

producing probabilities for each of the 527 acoustic classes contained in the Audio Set balanced training set. These PANNs models were trained using a balanced set of Audio Set training data; we did no further training on the models. In total, all PANN models were evaluated on 6 acoustic tasks where they showed good performance, thus validating their robustness for learning inherent structure in audio signals. Furthermore, performance and computational complexity trade-offs are studied for these models [27]. The state-of-the-art models were trained on 527 classes contained in Audioset balanced set. We chose to not retrain the state-of-the-art models for binary classification as most of these models were too large model size and long inference time making them unsuitable for integration in product stack. Secondly, we were able to achieve high TPR at very low FPR with proposed MusicNet architecture eliminating the need for neural architecture search based on state-of-the-art models which would require re-training.

3.5.1. Featurization MusicNet takes 16 kHz single-channel raw audio waveform and computes the 120-band Mel spectrogram for each 20 ms frame with a hop size of 10 ms. While inference runtimes like TensorFlow Lite and ONNX Runtime help with the model portability, porting the featurization pipeline, in general, remains a challenge. In particular, STFT implementations in the popular Python libraries like numpy, librosa, and TensorFlow all have different defaults for the parameters like windowing and padding, or require support for complex numbers. This leads to code portability issues and often leads to numerical mismatches between the Python featurization in training and its C++ implementation in the product. In addition, integration and maintenance of the featurization code creates barriers for iterative development of new production models. To facilitate easier integration, maintenance and updates to the production model, we build the featurization as part of the model. We implemented featurization as 1-D convolutional layer in PyTorch. We explained the data flow in the featurization layer in Section 3.4.
The in-model featurization enables a portable ONNX model which takes raw 9 second audio and outputs the probability of music.

3.4. Proposed Model We built a binary music classifier for detecting music in 9 second clips. To this end, we explored different configurations of the CNN-based models. The MusicNet architecture for our best-performing model is shown in Table 1. The input to the model is a 9 second raw audio waveform. The first layer of the model is the featurization layer which converts the raw waveform to real and imaginary parts of the spectrum, computes the absolute magnitude of the spectrum, multiplies it with the Mel weights to get the Mel-spectrum, and finally takes the log to get the LogMel spectrum. The audio frames from the 9 second clips are stacked so that we have 900 x 320 dimensions for raw waveform (considering the overlap of 160 samples) input, which is fed to two 1-D convolutional layers, one each for the real and imaginary part of the spectrum.
In the featurization layer, we learn two weight matrices to convert the raw waveform to real and imaginary parts of the spectrum and keep the Mel-weights fixed. We used the librosa Mel-weights in our model. We take log power Mel-spectrogram with 120 Mel bands computed over a 9 second clip sampled at 16 kHz with a frame size of 20 ms and a hop length of 10 ms.
This results in an input dimension of 900 x 120 where each 1 x 120 corresponds to a frame-size 20ms obtained by sliding the window by 10s. We train the model using a batch size of 32 with the Adam optimizer and binary cross-entropy loss function until the loss is saturated. The in-model featurization is introduced to make it easier for integration in product stack. It allows model

4. Results & Discussions We compare MusicNet to 20 PANN models from [27]. To make the comparison fair, we convert PANN models using the code from [28] to the ONNX format and benchmark them on ONNX Runtime v1.1, which is the version we use on client devices in production. Our use case requires a high TPR at a low FPR=0.1%. However, the overall AUC score is of less importance as our operating point is decided based on the target application and how many false-positives per hour are acceptable.
Classifiers like Cnn14 DecisionLevelMax have an AUC=0.99 but TPR=7.3% at FPR=0.1%, which is unacceptable performance and a good example why using AUC to select music classifiers is insufficient. To ensure the best customer experience, an FPR=0.1% is chosen which translates to one false de3 https://onnx.ai/

4164

Table 2: Inference time per 10-second audio frame, ONNX file size, and accuracy of the models in the study.
Model Proposed Model Wavegram Cnn14 Cnn14 DecisionLevelMax Cnn14 emb512 Cnn14 emb128 Cnn14 emb32 Cnn14 Cnn10 Cnn6 Res1dNet51 Res1dNet31 ResNet54 ResNet38 ResNet22 DaiNet19 LeeNet24 LeeNet11 MobileNetV2 MobileNetV1

Inference time, ms

Size,
MB

AUC

0.2 308.9

TPR at 0.1%
FPR 81.3%
61.8%

11.1 285.2 249.4 242.4 239.6 240.8 240.1 163.2 140.7 480.0 271.8 340.8 282.3 171.2 238.7 212.2 44.1 14.4 15.1

308.1 293.0 289.2 288.3 308.1 19.9 18.5 406.7 307.2 398.2 281.6 243.0 16.8 38.2 2.9 15.7 18.4

7.3%
67.6%
40.9%
57.9%
51.7%
48.9%
80.7%
23.8%
24.9%
75.3%
46.4%
62.9%
45.7%
28.1%
25.5%
46.1%
52.4%

0.99 0.98 0.99 0.99 0.99 0.99 0.98 0.94 0.93 0.99 0.98 0.98 0.95 0.93 0.95 0.96 0.98

ground noises.
Future work can include optimizing the MusicNet model to achieve TPR ≥ 95% at FPR=0.1% while further reducing the inference time. Another interesting step would be collecting a more diverse real test set with 5000 clips.

0.97 0.95

6. References
[1] J. F. Gemmeke, D. P. W. Ellis, D. Freedman, A. Jansen,
W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, “Audio set: An ontology and human-labeled dataset for audio events,”
in ICASSP, 2017.
[2] I. Papadimitriou, A. Vafeiadis, A. Lalas, K. Votis, and D. Tzovaras, “Audio-based event detection at different snr settings using two-dimensional spectrogram magnitude representations,” Electronics, 2020.
[3] S. Deshmukh, B. Raj, and R. Singh, “Multi-task learning for interpretable weakly labelled sound event detection,” arXiv preprint arXiv:2008.07085, 2020.
[4] H. Dubey, D. Emmanouilidou, and I. J. Tashev, “Cure dataset:
Ladder networks for audio event classification,” in IEEE Pacific Rim Conference on Communications, Computers and Signal Processing (PACRIM), 2019.
[5] J. Sohn, N. S. Kim, and W. Sung, “A statistical model-based voice activity detection,” IEEE signal processing letters, 1999.
[6] X. Zhang and D. Wang, “Boosting contextual information for deep neural network based voice activity detection,” IEEE/ACM Trans. on Audio, Speech, and Language Processing, 2015.
[7] K. Seyerlehner, T. Pohle, M. Schedl, and G. Widmer, “Automatic music detection in television productions,” in Proc. of the 10th International Conference on Digital Audio Effects (DAFx’07), 2007.

tection of the music events every 3 hours. As Table 2 shows,
at the chosen operating point of FPR=0.1% MusicNet achieves the best performance of TPR=81.3%.
Table 2 also shows the inference time for 9-second audio clips measured on Intel i7-1065G7 CPU running Windows 10 and ONNX Runtime 1.1. MusicNet is faster than the best PANN model, and also is the smallest with a size of 0.22MB for an uncompressed ONNX file.
The key reasons why MusicNet performs better than other methods are: (1) We use 9 seconds of raw audio to extract 120 bands Mel spectrogram. Other methods use shorter audio lengths with fewer Mel bands that work well for speech, but not for detecting music. (2) We avoid any kind of batch normalization in our network as it makes the network insensitive to level variations in the embedding space, which is important to detect music. It also helps the model generalize better on challenging and realistic test set. (3) We took reasonable steps to ensure the generalizability of the model by using the right amount of dropout, filter lengths and the number of filters. MusicNet is much smaller than MobileNet V1 and V2 in terms of the memory and yet achieves better accuracy.

[8] T. Izumitani, R. Mukai, and K. Kashino, “A background music detection method based on robust feature extraction,” in IEEE ICASSP, 2008.
[9] D. de Benito-Gorron, A. Lozano-Diez, D. Toledano, and J. Gonzalez-Rodriguez, “Exploring convolutional, recurrent, and hybrid deep neural networks for speech and music detection in a large audio dataset,” EURASIP Journal on Audio, Speech, and Music Processing, 2019.
[10] B. Jia, J. Lv, X. Peng, Y. Chen, and S. Yang, “Hierarchical regulated iterative network for joint task of music detection and music relative loudness estimation,” IEEE/ACM Trans. on Audio,
Speech, and Language Processing, 2020.
[11] D. Stowell, D. Giannoulis, E. Benetos, M. Lagrange, and M. D.
Plumbley, “Detection and classification of acoustic scenes and events,” IEEE Trans. on Multimedia, 2015.
[12] A. Temko, R. Malkin, C. Zieger, D. Macho, C. Nadeu, and M. Omologo, “Clear evaluation of acoustic event detection and classification systems,” in International Evaluation Workshop on Classification of Events, Activities and Relationships, 2006.
[13] Q. Kong, Y. Xu, W. Wang, and M. D. Plumbley, “Audio set classification with attention model: A probabilistic perspective,” in ICASSP, 2018.

5. Conclusions

[14] Q. Kong, C. Yu, Y. Xu, T. Iqbal, W. Wang, and M. D. Plumbley, “Weakly labelled audioset tagging with attention neural networks,” IEEE/ACM Trans. on Audio, Speech, and Language Processing, 2019.

We present a model for real-time background music detection and compare it with 20 other state-of-the-art models. MusicNet is 10x smaller than the next smallest model in the study and is 35% faster than the fastest PANN classifier. MusicNet achieves the best TPR=81.3% at an FPR=0.1% operating point.
Our model is portable to a wide range of client devices and incorporates Mel-spectrogram featurization as part of the model definition. To validate the model performance in our use case,
we created a representative test set of music clips from different instruments and genres with and without speech and back-

[15] C. K. A. Reddy, H. Dubey, K. Koishida, A. Nair, V. Gopal, R. Cutler, S. Braun, H. Gamper, R. Aichner, and S. Srinivasan, “INTERSPEECH 2021 deep noise suppression challenge,” in INTERSPEECH, 2021.
[16] Y. Xia, S. Braun, C. K. Reddy, H. Dubey, R. Cutler, and I. Tashev,
“Weighted speech distortion losses for neural-network-based realtime speech enhancement,” in IEEE ICASSP, 2020.

4165

“Artificially synthesising data for audio classification and segmentation to improve speech and music detection in radio broadcast,”
in ICASSP, 2021.

[17] C. K. Reddy, E. Beyrami, H. Dubey, V. Gopal, R. Cheng, R. Cutler, S. Matusevych, R. Aichner, A. Aazami, S. Braun et al., “The interspeech 2020 deep noise suppression challenge: Datasets,
subjective speech quality and testing framework,” in ISCA INTERSPEECH, 2020.

[24] K. El-Maleh, M. Klein, G. Petrucci, and P. Kabal, “Speech/music discrimination for multimedia applications,” in ICASSP, 2000.

[18] H. Dubey, V. Gopal, R. Cutler, A. Aazami, S. Matusevych,
S. Braun, S. E. Eskimez, M. Thakker, T. Yoshioka, H. Gamper et al., “ICASSP 2022 deep noise suppression challenge,” in IEEE ICASSP, 2022.

[25] G. Sell and P. Clark, “Music tonality features for speech/music discrimination,” in ICASSP, 2014.

[19] “MIREX 2018: Music and/or Speech Detection Challenge,” Music Information Retrieval Evaluation eXchange (MIREX), 2018.

[26] E. Fonseca, J. Pons Puig, X. Favory, F. Font Corbera, D. Bogdanov, A. Ferraro, S. Oramas, A. Porter, and X. Serra, “Freesound datasets: a platform for the creation of open audio datasets,” in ISMIR, 2017.

[20] B.-Y. Jang, W.-H. Heo, J.-H. Kim, and O.-W. Kwon, “Music detection from broadcast contents using convolutional neural networks with a mel-scale kernel,” EURASIP Journal on Audio,
Speech, and Music Processing, 2019.

[27] Q. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang, and M. D. Plumbley, “PANNs: Large-scale pretrained audio neural networks for audio pattern recognition,” IEEE/ACM Trans. on Audio, Speech,
and Language Processing, vol. 28, pp. 2880–2894, 2020.

[21] B. Jang, W. Heo, J. Kim, and O. Kwon, “Music and/or speech detection methods for MIREX 2018,” Music Information Retrieval Evaluation eX-change (MIREX), 2018.
[22] B. Meléndez-Catalán, E. Molina, and E. Gomez, “Music and/or speech detection mirex 2018 submission,” Music Information Retrieval Evaluation eX-change (MIREX), 2018.

[28] Q. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang, and M. Plumbley, “Github repos for PANNs: Large-scale pretrained audio neural networks for audio pattern recognition,” https://music-ir.
org/mirex/wiki/2018:Music and/or Speech Detection, 2020 (accessed Oct. 3, 2021).

[23] S. Venkatesh, D. Moffat, A. Kirke, G. Shakeri, S. Brewster,
J. Fachner, H. Odell-Miller, A. Street, N. Farina, S. Banerjee et al.,

4166


