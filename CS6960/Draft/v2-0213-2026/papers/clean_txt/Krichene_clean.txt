.

.

Published: 23 August 2020
.

.

.

Latest updates: hî€¼ps://dl.acm.org/doi/10.1145/3394486.3403226

.

.

.

PDF Download 3394486.3403226.pdf 14 February 2026 Total Citations: 375 Total Downloads: 18003

.

RESEARCH-ARTICLE

.

WALID KRICHENE, Google LLC, Mountain View, CA, United States

.

KDD '20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining July 6 - 10, 2020 CA, Virtual Event, USA

.

.

.

.

.

STEFFEN RENDLE, Google LLC, Mountain View, CA, United States

Citation in BibTeX format
.

On Sampled Metrics for Item Recommendation

.

Open Access Support provided by:
.

Google LLC

Conference Sponsors:
SIGMOD SIGKDD

KDD '20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (August 2020)
hî€¼ps://doi.org/10.1145/3394486.3403226 ISBN: 9781450379984

Research Track Paper

KDD '20, August 23â€“27, 2020, Virtual Event, USA

On Sampled Metrics for Item Recommendation Walid Krichene

Steffen Rendle

walidk@google.com Google Research Mountain View, California

srendle@google.com Google Research Mountain View, California

ABSTRACT

studies and often many millions in industrial applications. Finding matching items from this large pool is challenging as the user will usually only explore a few of the highest ranked ones. For evaluating recommender algorithms, usually sharp metrics such as precision or recall over the few highest scoring items (e.g., top 10) are chosen.
Another popular class are smooth metrics such as average precision or normalized discounted cumulative gain (NDCG) which place a strong emphasis on the top ranked items.
Recently, it has become common in research papers to speed up evaluation by sampling a small set of irrelevant items and ranking the relevant documents only among this smaller set [7, 9, 10, 12, 15â€“
17]. Sampling of negatives is commonly used during training of large models [4, 13], and several works have studied the implications of sampling as well as various methods to improve it [5, 6],
see [18] for a comparative study of sampling methods. However,
to the best of our knowledge, the implications of sampling during evaluation have not been explored. In this work, the consequences of this approach are studied. In particular, it is shown that findings from sampled metrics (even in expectation) can be inconsistent with exact metrics. This means that if a recommender A outperforms a recommender B on the sampled metric, it does not imply that A has a better metric than B when the metric is computed exactly. This is even a problem in expectation; i.e., with unlimited repetitions of the measurement. Moreover, a sampled metric has different characteristics than its exact counterpart. In general, the smaller the sampling size, the less differences there are between different metrics, and in the small sample limit, all metrics collapse to the area under the ROC curve, which discounts positions linearly.
This is particularly problematic because many ranking metrics are designed to focus on the top positions.
As we will show, the sampled metrics can be viewed as high-bias,
low-variance estimators of the exact metrics. Their low variance can be particularly misleading if one does not recognize that they are biased, as repeated measurements may indicate a low variance,
and yet no meaningful conclusion can be drawn because the bias is recommender-dependent, i.e. the value of the bias depends on the recommender algorithm being evaluated. We also show that this issue can be alleviated if one applies a point-wise correction to the sampled metric, by minimizing criteria that trade-off bias and variance. Empirical performance of the sampled metrics and their corrections is illustrated on a movie recommendation problem.
This analysis suggests that if a study is really interested in metrics that emphasize the top ranked items, sampling candidates should be avoided for the purposes of evaluation, and if the size of the problem is such that sampling is necessary, corrected metrics can provide a more accurate evaluation. Lastly, if sampling is used,
the reader should be aware that the reported metric has different characteristics than its name implies.

The task of item recommendation requires ranking a large catalogue of items given a context. Item recommendation algorithms are evaluated using ranking metrics that depend on the positions of relevant items. To speed up the computation of metrics, recent work often uses sampled metrics where only a smaller set of random items and the relevant items are ranked. This paper investigates sampled metrics in more detail and shows that they are inconsistent with their exact version, in the sense that they do not persist relative statements, e.g., recommender A is better than B, not even in expectation. Moreover, the smaller the sampling size, the less difference there is between metrics, and for very small sampling size, all metrics collapse to the AUC metric. We show that it is possible to improve the quality of the sampled metrics by applying a correction, obtained by minimizing different criteria such as bias or mean squared error. We conclude with an empirical evaluation of the naive sampled metrics and their corrected variants. To summarize, our work suggests that sampling should be avoided for metric calculation, however if an experimental study needs to sample, the proposed corrections can improve the quality of the estimate.

CCS CONCEPTS
â€¢ Information systems â†’ Recommender systems; Evaluation of retrieval results; â€¢ Computing methodologies â†’ Ranking.

KEYWORDS Item Recommendation; Evaluation; Metrics; Sampled Metric ACM Reference Format:
Walid Krichene and Steffen R endle. 2 020. O n S ampled M etrics f or Item Recommendation. In Proceedings of the 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD â€™20), August 23â€“27, 2020, Virtual Event, CA, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.
1145/3394486.3403226

1 INTRODUCTION Over recent years, item recommendation from implicit feedback has received a lot of attention from the recommender system research community. At its core, item recommendation is a retrieval task,
where given a context, a catalogue of items should be ranked and the top scoring ones are shown to the user. Usually the catalogue of items to retrieve from is large: tens of thousands in academic

This work is licensed under a Creative Commons Attribution International 4.0 License.
KDD â€™20, August 23â€“27, 2020, Virtual Event, CA, USA
Â© 2020 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-7998-4/20/08.
https://doi.org/10.1145/3394486.3403226

1748

Research Track Paper

KDD '20, August 23â€“27, 2020, Virtual Event, USA

1.0

AUC Average Precision NDCG Recall at 20 Precision at 20

0.8 Quality metric

Quality metric

0.8

1.0

0.6 0.4 0.2

AUC Average Precision NDCG Recall at 20 Precision at 20

0.6 0.4 0.2

0.0

0.0

0

2000

4000 6000 Rank r of relevant item

8000

10000

0

20

40 60 Rank r of relevant item

80

100

Figure 1: Visualization of metric vs. predicted rank for ğ‘› = 10, 000. The left side shows the metrics over the whole set of 10, 000 items. The right side zooms onto the contributions of the top 100 ranks. All metrics besides AUC are top heavy and almost completely ignore the tail. This is usually a desirable property for evaluating ranking because users are unlikely to explore items further down the result list.

2

EVALUATING ITEM RECOMMENDATION

with the indicator function ğ›¿ (ğ‘) = 1 if ğ‘ is true and 0 otherwise.
Precision at position ğ‘˜ measures the fraction of relevant items among the top ğ‘˜ predicted items:

This section starts by formalizing the most common evaluation scheme for item recommendation. Let there be a pool of ğ‘› items to recommend from. For a given instance1 x, a recommendation algorithm, ğ´, returns a ranked list of the ğ‘› items. In an evaluation,
the positions, ğ‘…(ğ´, x) âŠ† {1, . . . , ğ‘›}, of the withheld relevant items within this ranking are computed â€“ ğ‘… will also be referred to as the predicted ranks. For example, ğ‘…(ğ´, x) = {3, 5} means for an instance x recommender ğ´ ranked two relevant items at positions 3 and 5. Then, a metric ğ‘€ is used to translate the positions into a single number measuring the quality of the ranking. This process is repeated for a set of instances, ğ· = {x1, x2, . . .}, and an average metric is reported:
1 Ã•
ğ‘€ (ğ‘…(ğ´, x)).
(1)
|ğ· |

Recall(ğ‘…)ğ‘˜ =

AP(ğ‘…)ğ‘˜ =

NDCG(ğ‘…)ğ‘˜ = Ã

ğ‘˜
Ã•
1
ğ›¿ (ğ‘– âˆˆ ğ‘…)Prec(ğ‘…)ğ‘– .
min(|ğ‘…|, ğ‘˜) ğ‘–=1

ğ‘˜
Ã•

1

min( |ğ‘… |,ğ‘˜)
1
ğ‘–=1 log2 (ğ‘–+1) ğ‘–=1

3.1

(5)

ğ›¿ (ğ‘– âˆˆ ğ‘…)

1
. (6)
log2 (ğ‘– + 1)

Simplified Metrics

The remainder of the paper analyzes these metrics for |ğ‘…| = 1, i.e.,
there exists exactly one relevant item which is ranked at position ğ‘Ÿ .
This will simplify the analysis and give a better understanding of the differences between these metrics. The metrics of the previous section simplify to the following:

This section recalls commonly used metrics for measuring the quality of a ranking. For convenience, the arguments, ğ´, x, from ğ‘…(ğ´, x)
are omitted whenever the particular recommender, ğ´, or instance,
x, is clear from context. Instead, the shorter form ğ‘… is used.
Area under the ROC curve (AUC) measures the likelihood that a random relevant item is ranked higher than a random irrelevant item.
Ã•
Ã•
1 AUC(ğ‘…)ğ‘› =
ğ›¿ (ğ‘Ÿ < ğ‘Ÿ â€² )
|ğ‘…|(ğ‘› âˆ’ |ğ‘…|)
â€²

AUC(ğ‘Ÿ )ğ‘› =

ğ‘› âˆ’ğ‘Ÿ
,
ğ‘›âˆ’1

(7)

1 Prec(ğ‘Ÿ )ğ‘˜ = ğ›¿ (ğ‘Ÿ â‰¤ ğ‘˜) ,
ğ‘˜
Recall(ğ‘Ÿ )ğ‘˜ = ğ›¿ (ğ‘Ÿ â‰¤ ğ‘˜),
1 AP(ğ‘Ÿ )ğ‘˜ = ğ›¿ (ğ‘Ÿ â‰¤ ğ‘˜) ,
ğ‘Ÿ

ğ‘Ÿ âˆˆğ‘… ğ‘Ÿ âˆˆ ( {1,...,ğ‘› }\ğ‘…)

Ã
|ğ‘… |âˆ’1
ğ‘› âˆ’ 2 âˆ’ |ğ‘…1 | ğ‘Ÿ âˆˆğ‘… ğ‘Ÿ
,

(4)

Normalized discounted cumulative gain (NDCG) at ğ‘˜ places an inverse log reward on all positions that hold a relevant item:

METRICS

ğ‘› âˆ’ |ğ‘…|

|{ğ‘Ÿ âˆˆ ğ‘… : ğ‘Ÿ â‰¤ ğ‘˜ }|
.
|ğ‘…|

Average Precision at ğ‘˜ measures the precision at all ranks that hold a relevant item:

This problem definition assumes that in the ground truth, all relevant items are equally preferred by the user, i.e., that the relevant items are a set. This is the most commonly used evaluation scheme in recommender systems. In more complex cases, the ground truth includes preferences among the relevant items. For example, the ground truth can be a ranked list or weighted set. Our work shows issues with sampling in the simpler setup, which implies that the issues carry over to the more complex case.

=

(3)

Recall at position ğ‘˜ measures the fraction of all relevant items that were recovered in the top ğ‘˜:

xâˆˆğ·

3

|{ğ‘Ÿ âˆˆ ğ‘… : ğ‘Ÿ â‰¤ ğ‘˜ }|
.
ğ‘˜

Prec(ğ‘…)ğ‘˜ =

(2)

NDCG(ğ‘Ÿ )ğ‘˜ = ğ›¿ (ğ‘Ÿ â‰¤ ğ‘˜)

1 E.g., a user, context, or query.

1749

1
.
log2 (ğ‘Ÿ + 1)

(8)
(9)
(10)
(11)

Research Track Paper

KDD '20, August 23â€“27, 2020, Virtual Event, USA

A B C

Predicted Ranks 100, 100, 100, 100, 100 40, 40, 8437, 9266, 4482 212, 2, 743, 5342, 1548

AUC 0.990 0.555 0.843

AP 0.010 0.010 0.101

NDCG 0.150 0.122 0.208

Recall@10 0.000 0.000 0.200

Table 1: Toy example of evaluating three recommenders A, B and C on five instances.

A B C

Predicted Ranks 100, 100, 100, 100, 100 40, 40, 8437, 9266, 4482 212, 2, 743, 5342, 1548

AUC 0.990Â±0.004 0.555Â±0.014 0.843Â±0.014

AP 0.630Â±0.129 0.336Â±0.073 0.325Â±0.050

NDCG 0.724Â±0.097 0.444Â±0.054 0.460Â±0.039

Recall@10 1.000Â±0.000 0.400Â±0.000 0.567Â±0.092

Table 2: Sampled evaluation for the recommenders from Table 1. On sampled metrics, the relative ordering of A, B, C is not preserved, except for AUC.

For metrics such as Average Precision and NDCG, it makes sense to also define their untruncated counterpart, e.g., for ğ‘˜ = ğ‘›:
1
(12)
AP(ğ‘Ÿ ) = ,
ğ‘Ÿ
1 NDCG(ğ‘Ÿ ) =
.
(13)
log2 (ğ‘Ÿ + 1)

only on the ranking generated by this subset [7, 9, 10, 12, 15â€“17].
It is common to pick the number of sampled irrelevant items, ğ‘š, in the order of a hundred while the number of items ğ‘› is much larger,
e.g., ğ‘š = 100 samples for datasets with ğ‘› = {4ğ‘˜, 10ğ‘˜, 17ğ‘˜, 140ğ‘˜, 2ğ‘€ }
items [7, 9, 15], ğ‘š = 50 samples for ğ‘› âˆˆ {2ğ‘˜, 18ğ‘˜, 14ğ‘˜ } items [10], or
ğ‘š = 200 samples for ğ‘› âˆˆ {17ğ‘˜, 450ğ‘˜ } items [17]. This section will highlight that this approach is problematic. In particular, results can become inconsistent with the exact metrics.
Let ğ‘…Ëœ be the ranks of the relevant items among the union of relevant items and the ğ‘š randomly sampled irrelevant items. It is important to note that ğ‘…Ëœ is a random variable, i.e., it depends on the random sample of irrelevant items. The properties of ğ‘…Ëœ will be analyzed in Section 4.3.

Some other popular metrics can be reduced to these definitions:
For |ğ‘…| = 1, Reciprocal Rank is equivalent to Average Precision, Hit Ratio is equivalent to Recall and Accuracy is equivalent to Recall at 1, and Precision at 1.
Figure 1 visualizes how the different ranking metrics trade-off the position vs. quality score. Average precision has the sharpest score decay, e.g., rank 1 is twice as valuable as rank 2, whereas for NDCG, rank 1 is 1.58 more valuable than rank 2. The least positionaware metric is AUC which places a linear decay on the rank. E.g.,
pushing an item from position 101 to 100 is as valuable as pushing an item from position 2 to 1.

3.2

4.1

A central goal of evaluation metrics is to make comparisons between recommenders, such as, recommender ğ´ has a higher value than
ğµ on metric ğ‘€. When comparing recommenders among sampled metrics, we would hope that at least the relative order is preserved in expectation. This property can be formalized as follows.

Example

This section concludes with a short example that will be used throughout this work. Let there be three recommenders ğ´, ğµ, ğ¶
and a set of ğ‘› = 10, 000 items. Each recommender is evaluated on five instances (i.e., |ğ· | = 5) with one relevant item each. For each instance, each recommender creates a ranking and the position at which the relevant item appears is recorded. Assume that recommender ğ¶ manages to rank the relevant item in one of the evaluation instances on position 2, besides this it never achieves a good rank for the other four instances. Assume recommender ğµ
ranks relevant items in two evaluation instances at position 40. And recommender ğ´ is never good nor terrible and the relevant items are ranked at position 100 in each of the five instances. Table 1 shows more details about the predicted ranks and the corresponding evaluation metrics. On AUC, recommender ğ´ is the best as it cares about all ranks equally. For top heavy metrics (AP, NDCG and Recall), recommender ğ¶ scores the highest. This example will be revisited in Section 4.2 when sampled metrics are discussed.

4

Inconsistency of Sampled Metrics

Definition 1 (Consistency). Let the evaluation data ğ· be fixed.
A metric ğ‘€ is consistent under sampling if the relative order of any two recommenders ğ´ and ğµ is preserved in expectation. That is, for all ğ´, ğµ,
1 Ã•
1 Ã•
ğ‘€ (ğ‘…(ğ´, x)) >
ğ‘€ (ğ‘…(ğµ, x))
|ğ· |
|ğ· |
xâˆˆğ·
xâˆˆğ·
"
#
"
#
1 Ã•
1 Ã•
Ëœ
Ëœ
â‡â‡’ ğ¸
ğ‘€ (ğ‘…(ğ´, x)) > ğ¸
ğ‘€ (ğ‘…(ğµ, x)) . (14)
|ğ· |
|ğ· |
xâˆˆğ·

xâˆˆğ·

If a metric is inconsistent, then measuring ğ‘€ on a subsample is not a good indicator of the true performance of ğ‘€.

4.2

Example

Now, the example from Section 3.2 is revisited and the same measures are computed using sampling. Specifically, ğ‘š = 99 random irrelevant items are sampled, the position ğ‘ŸËœ of the relevant item among this sampled subset is found, and then the metrics are computed for the rank ğ‘ŸËœ within the subsample. This procedure with a comparable sample size is commonly used in recent work [7, 9, 10,
15, 17].

SAMPLED METRICS

Ranking all items is expensive when the number of items, ğ‘›, is large. Recently, it has become common to sample a small set of ğ‘š
irrelevant items, add the relevant items, and compute the metrics

1750

Recommender A Recommender B Recommender C

0.8 0.6 0.4 0.2 0

200

400 600 800 Number of Samples

1000

Number of Samples: NDCG

1.0

1.0

Recommender A Recommender B Recommender C

0.8 0.6 0.4 0.2

Number of Samples: Recall at 10

0.8 0.6 0.4 0.2

200

400 600 800 Number of Samples

1000

0.9 Recommender A Recommender B Recommender C

0.8 0.7 0.6

0.0 0

Number of Samples: AUC

1.0

Recommender A Recommender B Recommender C

Expected AUC

Number of Samples: Average Precision

Expected Recall at 10

1.0

KDD '20, August 23â€“27, 2020, Virtual Event, USA

Expected NDCG

Expected Average Precision

Research Track Paper

0

1000 2000 3000 4000 Number of Samples

5000

0

200

400 600 800 Number of Samples

1000

Figure 2: Expected sampling metrics for the running example (Section 3.2 and 4.2) while increasing the number of samples.
For Average Precision, NDCG and Recall, even the relative order of recommender performance changes with the number of samples. That means, conclusions drawn from a subsample are not consistent with the true performance of the recommender.
Table 2 shows the sampled metrics for the example from Section 3.2. As this is a random process, for better understanding of its outcome, here it is repeated 1000 times and the average and standard deviation is computed2 .
Compared to the exact metrics in Table 1, even the relative ordering of metrics completely changed. On the exact metrics, C is clearly the best with a 10x higher average precision than B and A. But it has the lowest average precision when sampled measurements are used. A and B perform the same on the exact metrics, but A has a 2x better average precision on the sampled metrics. Sampled average precision does not give any indication of the true ordering among the methods. Similarly, sampled NDCG and sampled Recall at 10 do not agree with the exact metrics. Only AUC is consistent between sampled and exact computation. The other metrics are inconsistent.
Figure 2 shows the same study as in the previous table, as we vary the number of samples, ğ‘š. The relative ordering of recommenders changes with an increasing sample size. For example, for average precision, depending on the number of samples, any conclusion could be drawn: A better than C better than B (for sample size < 50),
A better than B better than C (for sample size â‰ˆ 200), C better than A better than B (for sample size â‰ˆ 500), and finally C better than A equal B (for large sample sizes). This example shows that the bias of sampled average precision is recommender dependent and samplesize dependent. This is why the relative ordering of recommenders changes as we change the sample size. Similar observations can be made for NDCG. Recall is even more sensitive to the sample size,
and it takes about ğ‘š = 5, 000 samples out of ğ‘› = 10, 000 items for the metric to become consistent. Only AUC is consistent for all ğ‘š,
and the expected metric is independent of sample size.

4.3

ğ‘›, then the probability that the sampled item ğ‘— is ranked above ğ‘Ÿ is:
ğ‘(ğ‘— < ğ‘Ÿ) =

ğ‘Ÿ âˆ’1
.
ğ‘›âˆ’1

(15)

For example, if ğ‘Ÿ is at position 1, the likelihood of a random irrelevant being ranked higher is 0. If ğ‘Ÿ = ğ‘›, then the likelihood is 1.
Note that the pool of all possible sampled items excludes the truly relevant item and thus has size ğ‘› âˆ’ 1.
Repeating the sampling procedure ğ‘š times with replacement and counting how often an item is ranked higher, corresponds to a Binomial distribution. In other words,
ğ‘ŸËœ obtained from

 the rank
ğ‘Ÿ âˆ’1 + 1. If there are no the sampling process follows ğ‘ŸËœ âˆ¼ ğµ ğ‘š, ğ‘›âˆ’1 successes in getting a higher ranked item, the rank remains 1, if all
ğ‘š samples are successful, the rank is ğ‘š + 1. The expected value of the metrics under this distribution is

ğ¸ [ğ‘€ (ğ‘ŸËœ)] =

ğ‘š+1
Ã•

ğ‘ (ğ‘ŸËœ = ğ‘–)ğ‘€ (ğ‘–).

(16)

ğ‘–=1

Note that this is implicitly a function of ğ‘Ÿ, ğ‘š, which appear as parameters of the Binomial distribution. Figure 3 visualizes the expected metrics ğ¸ (ğ‘€ (ğ‘ŸËœ)) as we vary ğ‘Ÿ . The figure highlights the weight that the sampled metric assigns to different ranks. Metrics like Average Precision or NDCG are much less top heavy. Even sharp metrics such as recall become smooth. Only AUC remains unchanged. In general, all metrics converge to a linear function in the small sample limit, similar to AUC behavior.

4.4

Rank Distribution under Sampling

Expected Metrics

This section analyzes sampled metrics in a more formal way by applying eq. (16) to particular metrics. The discussion focuses on uniform sampling with replacement, i.e., Binomial distributed ranks.
Similar results hold for uniform sampling without replacement. In this case, the distribution is hypergeometric, with population size
ğ‘› âˆ’ 1, where a pool of ğ‘Ÿ âˆ’ 1 items can be potential successes. When appropriate, this variation will be discussed as well.

This section takes a closer look at the sampling process and derives the distribution of ranks, ğ‘…Ëœ and the expected metrics. For simplicity,
the analysis is restricted to rankings with exactly one relevant item,
Ëœ = 1, so we can use the simplified metrics from Section 3.1.
i.e., |ğ‘…|
Let ğ‘Ÿ denote the true rank of the unique relevant item, and ğ‘ŸËœ denote its measured rank on the sample.
When an irrelevant item is sampled uniformly, it can either rank higher or lower than the relevant item. If the number of all items is

4.4.1

2 In a real evaluation, the process would not be repeated because this would contradict

Expected AUC. First, AUC is a linear function of the rank:

AUCğ‘› (ğ‘Ÿ ) =

the motivation of sampling to reduce computational cost.

1751

ğ‘› âˆ’ğ‘Ÿ
1
ğ‘›
=âˆ’
ğ‘Ÿ+
= const1 ğ‘Ÿ + const2 . (17)
ğ‘›âˆ’1
ğ‘›âˆ’1
ğ‘›âˆ’1

KDD '20, August 23â€“27, 2020, Virtual Event, USA

m=1 m=10 m=100 m=1000 Exact

0.6 0.4 0.2

0.8

m=1 m=10 m=100 m=1000 Exact

0.6 0.4 0.2

0.0 0

200 400 600 800 Rank r of relevant item

1000

Sampled Recall at 5

1.0 0.8

m=1 m=10 m=100 m=1000 Exact

0.6 0.4 0.2

200 400 600 800 Rank r of relevant item

1000

0.98

m=1 m=10 m=100 m=1000 Exact

0.96 0.94 0.92

0.0 0

Sampled AUC

1.00

Expected AUC

0.8

Sampled NDCG

1.0

Expected Recall at 5

Sampled Average Precision

1.0

Expected NDCG

Expected Average Precision

Research Track Paper

0.90

0

200 400 600 800 Rank r of relevant item

1000

0

200 400 600 800 Rank r of relevant item

1000

Figure 3: Characteristics of a sampled metric with a varying number of samples. Sampled Average Precision, NDCG and Recall change their characteristics substantially compared to exact computation of the metric. Even large sampling sizes (ğ‘š = 1000 samples of ğ‘› = 10000 items) show large bias. Note this plot zooms into the top 1000 ranks out of ğ‘› = 10000 items.
Thus by linearity of the expectation, and the fact that ğ‘ŸËœ follows a Binomial distribution, we have


ğ‘Ÿ âˆ’1
ğ¸ [AUCğ‘š+1 (ğ‘ŸËœ)] = AUCğ‘š+1 (ğ¸ [ğ‘ŸËœ]) = AUCğ‘š+1 1 + ğ‘š
ğ‘›âˆ’1

relevant item is reasonably highly ranked (i.e., AUC is close to 1.0),
it takes many samples ğ‘š for this term to approach 0.
4.4.4 Small Sampling Size. This section investigates the behavior of sampled metrics in the limit, where ğ‘š = 1. In this case, ğ‘ŸËœ âˆˆ {1, 2},
and for any metric ğ‘€ and any sampling distribution:

ğ‘Ÿ âˆ’1
ğ‘š + 1 âˆ’ 1 âˆ’ ğ‘š ğ‘›âˆ’1
ğ‘› âˆ’ğ‘Ÿ
=
= AUCğ‘› (ğ‘Ÿ ).
ğ‘š+1âˆ’1
ğ‘›âˆ’1 That means AUC measurements created by sampling are unbiased estimators of the exact AUC. This result is not surprising because the AUC can alternatively be defined as the expectation that a random relevant item is ranked over a random irrelevant item.
Consequently, AUC is a consistent metric under sampling.
This result holds also for any sampling distributions where the
ğ‘Ÿ âˆ’1 . For example, this expected value of the sampled rank is 1 + ğ‘š ğ‘›âˆ’1 is also true for sampling from a hypergeometric distribution â€“ i.e.,
uniform sampling without replacement.

=

ğ¸ [ğ‘€ (ğ‘ŸËœ)] = ğ‘ (ğ‘ŸËœ = 1)ğ‘€ (1) + (1 âˆ’ ğ‘ (ğ‘ŸËœ = 1))ğ‘€ (2).
For uniform sampling3 of items, ğ‘ (ğ‘ŸËœ = 1) is the probability to
ğ‘›âˆ’ğ‘Ÿ . Now,
sample an item that is ranked after ğ‘Ÿ , i.e., is ğ‘›âˆ’1
ğ‘› âˆ’ğ‘Ÿ
ğ¸ [ğ‘€ (ğ‘ŸËœ)] =
(ğ‘€ (1) âˆ’ ğ‘€ (2)) + ğ‘€ (2)
ğ‘›âˆ’1
ğ‘€ (2) âˆ’ ğ‘€ (1) ğ‘› ğ‘€ (1) âˆ’ ğ‘€ (2)
=ğ‘Ÿ
+
= ğ‘Ÿ const1 + const2,
ğ‘›âˆ’1
ğ‘›âˆ’1 which is a linear function of the true rank ğ‘Ÿ , regardless of the metric.
If we only care about the ordering produced by two different metrics on a set of rankings (eq. 14), we can ignore const2 . Similarly, for const1 , only the sign matters when comparing two sets of ranking.
This sign of ğ‘€ (2) âˆ’ ğ‘€ (1) depends on how much ranking a relevant item at position 1 is preferred over ranking it at position 2. For metrics that cannot distinguish between the first and second position,
such as precision and recall at ğ‘˜ â‰¥ 2, the sampled metric is always constant and not useful at all. For any reasonable metric, const1 should be negative, i.e., ranking at position 1 gives a higher metric than position 2. To summarize, for ğ‘š = 1 all metrics give the same qualitative result in expectation. There is no reason to choose one metric over the other if we are only interested in relative statements such as â€œmetric of ğ´ is higher than metric of ğµ". Furthermore, the qualitative result with ğ‘š = 1 coincides with exhaustive AUC since
(i) all sampled metrics, including sampled AUC, are indistinguishable for ğ‘š = 1 as shown in this section, and (ii) sampled AUC is consistent with exhaustive AUC as shown in Section 4.4.1.
The discussion above shows that it does not make sense to choose different metrics for ğ‘š = 1; any sensible metric gives the same qualitative statement. A similar observation can be found in Figure 3 and 2 where all metrics behave similarly for small samples sizes.

4.4.2 Cut-off metrics. For a cutoff metric such as recall or precision:
ğ¸ [Recallğ‘˜ (ğ‘ŸËœ)] =

=

ğ‘š+1
Ã•

ğ‘ (ğ‘ŸËœ = ğ‘–)Recallğ‘˜ (ğ‘–) =

ğ‘–=1
ğ‘˜
Ã•

ğ‘š+1
Ã•

ğ‘ (ğ‘ŸËœ = ğ‘–)ğ›¿ (ğ‘– â‰¤ ğ‘˜)

ğ‘–=1



ğ‘Ÿ âˆ’1
ğ‘ (ğ‘ŸËœ = ğ‘–) = CDF ğ‘˜ âˆ’ 1; ğ‘š,
.
ğ‘›âˆ’1
ğ‘–=1

(18)

This analysis carries over to any sampling distribution, including the hypergeometric distribution.
4.4.3 Average Precision. For the expected value of sampled average precision, we distinguish two cases. If ğ‘Ÿ = 1, then ğ‘ŸËœ = 1 and the sampled metric is always equal to 1. If ğ‘Ÿ > 1, then ğ‘ ( ğ‘— < ğ‘Ÿ ) > 0 and
ğ¸ [AP(ğ‘ŸËœ)] =

ğ‘š+1
Ã•
ğ‘–=1

ğ‘ (ğ‘ŸËœ = ğ‘–)AP(ğ‘–) =

ğ‘š+1
Ã•

ğ‘ (ğ‘ŸËœ = ğ‘–)

1
ğ‘–

ğ‘–=1

ğ‘›âˆ’ğ‘Ÿ  ğ‘š+1 1 âˆ’ (1 âˆ’ ğ‘ ( ğ‘— < ğ‘Ÿ ))ğ‘š+1 1 âˆ’ ğ‘›âˆ’1
=
=
.
ğ‘ ( ğ‘— < ğ‘Ÿ ) (ğ‘š + 1)
(ğ‘Ÿ âˆ’ 1) ğ‘š+1
ğ‘›âˆ’1

(19)

Interestingly, this can be written as:


1 âˆ’ AUCğ‘› (ğ‘Ÿ )ğ‘š+1 ğ‘› âˆ’ 1 1 âˆ’ AUCğ‘› (ğ‘Ÿ )ğ‘š+1
=
const.
ğ‘Ÿ âˆ’1
ğ‘š+1
ğ‘Ÿ âˆ’1

5

CORRECTED METRICS

So far, we have shown that sampled metrics have different characteristics than the same metric on the full set of items. This section investigates whether we can design a sampled metric Ë†
ğ‘€, a function

1 and would be similar to If ğ´ğ‘ˆ ğ¶ğ‘› (ğ‘Ÿ )ğ‘š+1 â‰ˆ 0.0, this would be ğ‘Ÿ âˆ’1 the unsampled average precision metric. However, as soon as the

3 Here ğ‘š = 1, so it does not matter whether sampling is with or without replacement.

1752

Research Track Paper

KDD '20, August 23â€“27, 2020, Virtual Event, USA

from {1, . . . , ğ‘š + 1} to R, such that ğ‘€Ë† (ğ‘ŸËœ) provides a good estimate of ğ‘€ (ğ‘Ÿ ). We will consider different definitions of what a "good"
estimate is.

5.1

not essential when averaging over a large number of evaluation points. The monotonic constraint corresponds to the linear inequalities ğ‘€Ë† ğ‘ŸËœ +1 â‰¥ ğ‘€Ë† ğ‘ŸËœ for all ğ‘ŸËœ. In this case, problem (21) becomes an isotonic regression problem [2]. We will refer to this as Constrained Least Squares in the experiments.

Unbiased Estimator of the Rank

Our first approach is motivated by a simple observation. The sampled metrics that are commonly used are obtained by applying the exact metric ğ‘€ to the observed rank ğ‘ŸËœ, i.e. ğ‘€Ë† (ğ‘ŸËœ) = ğ‘€ (ğ‘ŸËœ). But ğ‘ŸËœ is a poor estimate of the true rank ğ‘Ÿ , in fact it always under-estimates it. Instead, one can measure the metric not on the observed rank ğ‘ŸËœ,
but on an unbiased estimator of ğ‘Ÿ . Recall from Section 4.3 that

ğ‘Ÿ âˆ’1 + 1. If we let ğ‘ := ğ‘Ÿ âˆ’1 , then an unbiased estimator
ğ‘ŸËœ |ğ‘Ÿ âˆ¼ ğµ ğ‘š, ğ‘›âˆ’1
ğ‘›âˆ’1

5.3

One potential issue with the minimal bias estimator is that it could have high variance, which we observe numerically in Section 6. In order to alleviate this problem, we can regularize problem (21) by introducing a variance term:
ğ‘›


Ã•
argmin
ğ‘ (ğ‘Ÿ ) (ğ¸ [ğ‘€Ë† ğ‘ŸËœ |ğ‘Ÿ ] âˆ’ ğ‘€ (ğ‘Ÿ )) 2 + ğ›¾ Var[ğ‘€Ë† ğ‘ŸËœ |ğ‘Ÿ ] ,
(24)

âˆ’1 . Thus an unbiased estimator of ğ‘Ÿ = 1 + (ğ‘› âˆ’ 1)ğ‘
of ğ‘ is given by ğ‘ŸËœğ‘š

is given by ğ‘ŸË† := 1 +
corrected metric:

ğ‘€Ë† âˆˆRğ‘š+1 ğ‘Ÿ =1

(ğ‘›âˆ’1) (ğ‘ŸËœ âˆ’1)
. This motivates using the following
ğ‘š



where ğ›¾ is a positive constant. This is a regularized least squares problem and its solution is given by:

 âˆ’1
ğ‘€Ë† = (1.0 âˆ’ ğ›¾)ğ´ğ‘‡ ğ´ + ğ›¾diag(c)
ğ´ğ‘‡ b,
(25)
Ãğ‘›
with ğ´ and b from eq. (23) and ğ‘ğ‘ŸËœ = ğ‘Ÿ =1 ğ‘ (ğ‘Ÿ )ğ‘ (ğ‘ŸËœ |ğ‘Ÿ ). When ğ›¾ = 0,
this reduces to problem (21). When ğ›¾ = 1, this reduces to the least squares estimator and the solution is
Ãğ‘›
Ã•
=1 ğ‘ (ğ‘ŸËœ |ğ‘Ÿ )ğ‘ (ğ‘Ÿ )ğ‘€ (ğ‘Ÿ )
ğ‘ (ğ‘Ÿ |ğ‘ŸËœ)ğ‘€ (ğ‘Ÿ ).
(26)
=
ğ‘€Ë† ğ‘ŸËœ = ğ‘ŸÃ
ğ‘› ğ‘ (ğ‘ŸËœ |ğ‘Ÿ )ğ‘ (ğ‘Ÿ )
ğ‘Ÿ =1
ğ‘Ÿ =1



(ğ‘› âˆ’ 1) (ğ‘ŸËœ âˆ’ 1)
ğ‘€Ë† (ğ‘ŸËœ) = ğ‘€ 1 +
.
ğ‘š

(20)

Since the rank estimate is a real number in [1, ğ‘›], and the original metric ğ‘€ is only defined on natural numbers, we can either round the rank estimate or extend ğ‘€ using e.g. linear interpolation. In our experiments, we round using floor âŒŠÂ·âŒ‹.

5.2

Minimal Bias Estimator

The first correction used an unbiased estimator of the rank. However, whenever ğ‘€ is nonlinear, ğ‘€Ë† (ğ‘ŸËœ) = ğ‘€ (ğ‘ŸË†) is biased in general. A criterion one may seek to optimize is the average bias of ğ‘€Ë† (ğ‘ŸËœ), that
Ã
is, ğ‘Ÿ ğ‘ (ğ‘Ÿ ) (ğ¸ [ğ‘€Ë† (ğ‘ŸËœ)|ğ‘Ÿ ] âˆ’ ğ‘€ (ğ‘Ÿ )) 2 , where ğ‘ (ğ‘Ÿ ) is a prior on the distribution of ranks, if available4 , or the uniform distribution otherwise.
Since ğ‘€Ë† is a function from {1, . . . , ğ‘š + 1} to R, ğ‘€Ë† can equivalently be viewed as a vector in Rğ‘š+1 . Thus we seek to find a vector ğ‘€Ë†
that minimizes the following problem:
ğ‘›
Ã•
argmin
ğ‘ (ğ‘Ÿ ) (ğ¸ [ğ‘€Ë† ğ‘ŸËœ |ğ‘Ÿ ] âˆ’ ğ‘€ (ğ‘Ÿ )) 2
(21)

In a real study, measurements are aggregated over many evaluation points, which reduces the overall variance, so a lower value ğ›¾ < 1 is preferable.

5.4

= argmin

!2
ğ‘ (ğ‘Ÿ )

ğ‘€Ë† âˆˆRğ‘š+1 ğ‘Ÿ =1

Ã•

ğ‘ (ğ‘ŸËœ |ğ‘Ÿ ) ğ‘€Ë† ğ‘ŸËœ âˆ’ ğ‘€ (ğ‘Ÿ )

Example

Figure 4 shows an example of a corrected average precision metric Ë†
AP, for several choices of the parameter ğ›¾, and for a uniform prior ğ‘ (ğ‘Ÿ ). The sample size is ğ‘š = 100 and the full item set is
ğ‘› = 10000, i.e., a sampling rate of 1%. As can be seen, when no order constraint is applied, lower values of ğ›¾ give oscillating solutions on the sample (left figure). This is not a problem in aggregate over the full evaluation set (right figure). All corrected sampled metrics are closer, in expectation, to the true metric.

ğ‘€Ë† âˆˆRğ‘š+1 ğ‘Ÿ =1
ğ‘›
Ã•

Bias-Variance Trade-off

.

ğ‘ŸËœ

This is a least squares problem, and its solution is given by

 âˆ’1
ğ‘€Ë† = ğ´ğ‘‡ ğ´
ğ´ğ‘‡ b,

5.5

Increasing the sample size ğ‘š reduces the bias of the sampled metrics,
as seen in Figure 3, as well as the corrected metrics: for example, the solution in (21) has a lower value when optimizing over a higher dimensional vector ğ‘€Ë† âˆˆ Rğ‘š+1 . Increasing the size of the item set,
ğ‘›, has the opposite effect. Increasing the number of evaluation points, |ğ· |, decreases the variance of the average estimates. This mostly benefits the corrected metrics introduced in this section;
the uncorrected metrics are high-bias estimators which will have a large error even in the limit of zero variance.

(22)

where p
ğ´ğ‘Ÿ,ğ‘ŸËœ = ğ‘ (ğ‘Ÿ )ğ‘ (ğ‘ŸËœ |ğ‘Ÿ ),
p b âˆˆ Rğ‘› , ğ‘ğ‘Ÿ = ğ‘ (ğ‘Ÿ )ğ‘€ (ğ‘Ÿ ).

Effect of the Sample Size and Data Set Size

ğ´ âˆˆ Rğ‘›Ã—ğ‘š+1,

(23)

Note that the problem is under-determined when ğ‘š + 1 < ğ‘›, i.e. in general, one cannot obtain an unbiased estimator for all ğ‘Ÿ . This is consistent with the observation made in Section 4.4.4, that for the limit case ğ‘š = 1, any metric coincides with (an affine transformation of) AUC.
It may also be desirable for the solution ğ‘€Ë† to be monotone nonincreasing, so that on any given evaluation point, a higher rank ğ‘ŸËœ
results in a lower estimated metric ğ‘€Ë† ğ‘ŸËœ , although this constraint is

6

EXPERIMENTS

In this section, we study sampled metrics on real recommender algorithms and a real dataset. We investigate: (1) Do recommender algorithms create different ranking distributions, e.g., some are better in the top, some are better overall? (2) Are results from sampled metrics and exact metrics inconsistent, e.g., a given recommender is

4 Note that the true distribution of ranks ğ‘ (ğ‘Ÿ ) is algorithm dependent and typically

unknown. We use a uniform prior in our experiments.

1753

Research Track Paper

KDD '20, August 23â€“27, 2020, Virtual Event, USA

Sampled Average Precision

0.8

Metric APÌ‚

0.6 0.4 0.2 0.0

1.0 Expected Average Precision

sampled (uncorrected)
Rank Estimate Constr. LS Bias+Var Bias+0.1*Var Bias+0.01*Var

Exact Ranks on Movielens

sampled (uncorrected)
Rank Estimate Constr. LS Bias+Var Bias+0.1*Var Bias+0.01*Var exact

0.8 0.6 0.4

0

20

40

Rank r Ìƒ

60

80

100

200

400 Rank r

600

better on the sampled metric but worse on the true metric? (3) Can corrections help to get more reliable results?
We use an identical experimental setup as [9] â€“ in particular the same dataset (binarized Movielens 1M [8]), split (holdout last event), sampling size (ğ‘š = 100) and metrics (Recall@10=HR@10 and NDCG@10). In addition, we report AP and AUC. We study the behavior of sampled metrics on three popular recommender system algorithms: matrix factorization and two variations of item-based collaborative filtering (see Section 9 in the appendix for details).
We want to emphasize that the purpose of our study is not to judge if a particular recommender algorithm is good. The purpose is rather to assess the behavior of metrics and correction methods.
To de-emphasize the particular recommender method and hyperparameter choice, we will refer to matrix factorization as â€˜recommender Xâ€™, to the two item-based collaborative filtering variations as â€˜recommender Yâ€™ and â€˜recommender Zâ€™.

1000 800 600

800

1000

0

100

101 102 Rank r of relevant item (log scale)

103

Figure 5: Distribution of predicted ranks for three recommender algorithms on the Movielens 1M dataset.

metrics. Also for NDCG and AP, the worst recommender on the exact metric (X) appears to be the best according to sampled metrics.
The relative ordering of the two better recommenders is correct.
For AUC, all sampled results are consistent with the exact metrics.
These results indicate that sampled metrics can be inconsistent in real experiments. In particular, if a study would have compared the recommenders only on the sampled metrics, the study would have drawn the wrong conclusion about the performance of the recommender with respect to top heavy metrics such as Recall,
NDCG and AP. The worst recommender (X) would have been found to be the best one.

6.3

Corrected Metrics

We finally investigate if correction methods can help. We consider the three correction strategies proposed in Section 5: rank estimate (eq. 20), constrained least squares (CLS) (eq. 21) with the constraint ğ‘€Ë† ğ‘ŸËœ â‰¥ ğ‘€Ë† ğ‘ŸËœ +1 , and bias-variance trade-off (BV ğ›¾) with ğ›¾ âˆˆ
{1.0, 0.1, 0.01, 0.001} and a uniform rank distribution ğ‘ (ğ‘Ÿ ) = 1/ğ‘›.
The right block of Table 3 shows the expected metrics under correction. All methods are closer to the exact results than sampling without correction. In particular, CLS and BV with low ğ›¾ have values close to the exact metric â€“ which indicates a low bias. All identify the order better, e.g., all of them place recommender Z as the best performing method for Recall, NDCG and AP. Some of them (BV with low ğ›¾) also get the order of recommenders X and Y right. Figure 6 shows the expected Recall@10 for different choices of the sampling size ğ‘š. As we can see, the uncorrected metric performs poorly and needs more than ğ‘š = 1000 samples
(equivalent to 1/3rd sampling rate) to correctly order recommenders X and Y. The corrected metric using a bias-variance trade-off with
ğ›¾ = 0.1 already has the correct ordering with less than ğ‘š = 60 samples.
While the corrections seem to be effective in expectation, one also needs to consider the variance of these measurements. Table 4 investigates the bias and variance in more detail. For each sampled metric and each pair of recommenders, we compare the order of the pair over the 100 runs, and count how often the order is correct,
i.e. agrees with that of the exact metric. For example, for Recall and "X vs Y" we count in how many of the 100 runs, the metric of recommender X is worse than Y. Table 4 shows that the correction methods are able to resolve most of the mistakes of the uncorrected

Rank Distributions

For each of the 6040 test users, we rank all items (leaving out the userâ€™s training items) and record at which position the withheld relevant item appears. In total we get 6040 ranks. Figure 5 shows the distribution of these ranks. The plot indicates the different characteristics of the three recommenders. Z is the best in the top 10 but has very poor performance at higher ranks as it puts the relevant items of over 1600 users in the worst bucket. X is more balanced and puts only few items at poor ranks; 2310 items are in the top 100 and less than 300 are in the bottom half. Y is in the middle, with a better top 10 performance than X, but tends to put the relevant item at a worse rank overall.

6.2

1200

200

0

Ë† on a sample of ğ‘š = 100 items (left)
Figure 4: Evaluating the corrected metric AP is equivalent to measuring the metric on the full item set of ğ‘š = 10, 000 (right).
Different choices of correction algorithms are plotted.

6.1

1400

400

0.2 0.0

âˆ’0.2

Recommender X Recommender Y Recommender Z

1600

Frequency

Corrected Metric on Samples 1.0

Sampled Metrics

The leftmost block in Table 3 reports the exact metric and the sampled metric with standard deviation5 . As expected from the rank distributions, for Recall, NDCG and AP, recommender Z is better than Y better than X on the exact metric. However, on the sampled metric this does not hold. For sampled Recall, the order is reversed and recommender X is much better than Y which is better than Z. All the measures have low standard deviation, so the issue is not that of variance, but is due to the bias in the sampled 5 We repeated the sampling experiment 100 times to measure the variance.

1754

AUC

AP

NDCG

Recall

Research Track Paper

KDD '20, August 23â€“27, 2020, Virtual Event, USA

Recommender X Y Z X Y Z X Y Z X Y Z

Exact 7.60 8.84 9.42 3.76 4.59 4.79 3.75 4.32 4.44 89.13 85.33 74.73

Sampled
(uncorrected)
66.19Â±0.25 56.51Â±0.22 54.20Â±0.22 39.21Â±0.20 34.82Â±0.16 35.34Â±0.16 32.55Â±0.21 30.01Â±0.20 30.71Â±0.21 89.12Â±0.04 85.33Â±0.04 75.04Â±0.23

Rank Estimate 17.46Â±0.32 17.26Â±0.28 18.67Â±0.32 17.46Â±0.32 17.26Â±0.28 18.67Â±0.32 18.12Â±0.31 17.81Â±0.28 19.20Â±0.31 89.24Â±0.04 85.48Â±0.04 75.24Â±0.20

Sampled with Correction CLS BV 1 BV 0.1 8.52Â±0.16 4.71Â±0.07 6.49Â±0.25 8.42Â±0.14 4.60Â±0.07 6.95Â±0.21 9.10Â±0.15 4.97Â±0.07 7.44Â±0.24 3.99Â±0.07 2.16Â±0.04 3.00Â±0.12 3.94Â±0.06 2.12Â±0.03 3.24Â±0.10 4.27Â±0.07 2.29Â±0.04 3.46Â±0.12 3.58Â±0.06 2.44Â±0.03 3.13Â±0.09 3.54Â±0.06 2.32Â±0.03 3.19Â±0.07 3.82Â±0.06 2.45Â±0.03 3.38Â±0.08 89.12Â±0.04 88.36Â±0.04 89.04Â±0.04 85.32Â±0.04 84.63Â±0.04 85.26Â±0.04 75.04Â±0.21 74.51Â±0.23 75.02Â±0.20

BV 0.01 7.18Â±0.59 8.18Â±0.51 8.63Â±0.59 3.34Â±0.32 3.85Â±0.28 4.05Â±0.32 3.37Â±0.21 3.62Â±0.18 3.79Â±0.21 89.11Â±0.04 85.32Â±0.04 75.02Â±0.24

BV 0.001 7.32Â±1.17 8.54Â±1.07 9.08Â±1.20 3.41Â±0.71 4.03Â±0.66 4.31Â±0.74 3.42Â±0.49 3.73Â±0.45 3.97Â±0.51 89.12Â±0.04 85.32Â±0.04 75.02Â±0.23

Table 3: Evaluation of three recommenders (X, Y and Z) on the Movielens dataset. Sampled metrics are inconsistent with the exact metrics. Corrected metrics, especially Bias2 +ğ›¾ âˆ—Variance with ğ›¾ â‰¤ 0.1 produce the correct relative ordering in expectation.
Movielens: Sampled (uncorrected)
1.0 0.9

Recommender X Recommender Y Recommender Z

0.07 0.06

0.8

Expected Recall at 10

Expected Recall at 10

Movielens: Bias 0.1*Variance Correction

Recommender X Recommender Y Recommender Z

0.7 0.6 0.5 0.4 0.3

0.05 0.04 0.03 0.02 0.01

0.2 0

200

400 600 Number of Samples (m)

800

1000

0

20

40 60 Number of Samples (m)

80

100

Figure 6: Evaluating recommenders with a varying sample size ğ‘š. Plots show expected Recall@10 for the uncorrected metric and the metric corrected by Bias2 + 0.1 âˆ— Variance. The uncorrected metric needs ğ‘š = 1000 samples to order X and Y correctly in expectation, while for the corrected metric requires only ğ‘š = 60.

Y vs Z

X vs Z

X vs Y

Sampled Sampled with Correction Measure (uncorrected) Rank Estimate CLS BV 1 BV 0.1 BV 0.01 BV 0.001 Recall 0 31 31 11 93 91 78 NDCG 0 31 31 15 93 88 76 AP 0 24 31 0 68 79 66 AUC 100 100 100 100 100 100 100 Recall 0 100 100 100 100 95 86 NDCG 0 100 100 100 100 95 82 AP 0 100 100 61 99 92 81 AUC 100 100 100 100 100 100 100 Recall 0 100 100 100 95 72 68 NDCG 100 100 100 100 94 70 67 AP 100 100 100 100 98 75 68 AUC 100 100 100 100 100 100 100 Table 4: For the 100 repetitions of the experiment in Table 3, how many times the metric for a pair of recommenders show the correct ordering. For example: for Recall and "X vs Y", how often the sampled metric of X was smaller than the sampled metric of Y. In any of the comparisons, a value of 100 indicates the evaluation was always correct, 0 indicates it was always wrong. The exact metric would always score 100.

1755

Research Track Paper

KDD '20, August 23â€“27, 2020, Virtual Event, USA

ACKNOWLEDGEMENTS

metrics. For this particular experiment, BV 0.1 seems to be the most effective one, getting the correct order on all but one comparison with >90% chance. The simple â€˜rank estimateâ€™ correction is surprisingly effective and is strictly better than the uncorrected metric in all comparisons of Table 4. It is worth mentioning that under the â€˜rank estimateâ€™ correction, Recall@10 and NDCG@10 are identical. However, it still represents an improvement over the uncorrected metrics which are much more biased and lead to the wrong conclusion. Rank estimate method is trivial to implement
(i.e., upscaling the rank before applying the metric). In a study with sampled evaluation, this should be preferred over uncorrected metrics. More complex corrections such as the adjusted bias-variance can get higher gains but are more difficult to implement.

7

We would like to thank Nicolas Mayoraz and Li Zhang for their helpful comments and suggestions.

REFERENCES
[1] Fabio Aiolli. 2013. Efficient Top-n Recommendation for Very Large Scale Binary Rated Datasets. In Proceedings of the 7th ACM Conference on Recommender Systems
(Hong Kong, China) (RecSys â€™13). Association for Computing Machinery, New York, NY, USA, 273â€“280. https://doi.org/10.1145/2507157.2507189
[2] R.E. Barlow, D.J. Bartholomew, J. M. Bremner, and Brunk H. D. 1972. Statistical Inference Under Order Restrictions: The Theory and Application of Isotonic Regression.
J. Wiley.
[3] Immanuel Bayer, Xiangnan He, Bhargav Kanagal, and Steffen Rendle. 2017. A Generic Coordinate Descent Framework for Learning from Implicit Feedback.
In Proceedings of the 26th International Conference on World Wide Web (Perth,
Australia) (WWW â€™17). 1341â€“1350. https://doi.org/10.1145/3038912.3052694
[4] Yoshua Bengio and Jean-SÃ©bastien Senecal. 2003. Quick Training of Probabilistic Neural Nets by Importance Sampling. In Proceedings of the Ninth International Workshop on Artificial Intelligence and Statistics, AISTATS 2003, Key West, Florida,
USA, January 3-6, 2003.
[5] Yoshua Bengio and Jean-SÃ©bastien Senecal. 2008. Adaptive Importance Sampling to Accelerate Training of a Neural Probabilistic Language Model. IEEE Trans.
Neural Networks 19, 4 (2008), 713â€“722.
[6] Guy Blanc and Steffen Rendle. 2018. Adaptive Sampled Softmax with Kernel Based Sampling. In Proceedings of the 35th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 80), Jennifer Dy and Andreas Krause (Eds.). PMLR, StockholmsmÃ¤ssan, Stockholm Sweden, 590â€“599.
[7] Travis Ebesu, Bin Shen, and Yi Fang. 2018. Collaborative Memory Network for Recommendation Systems. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval (Ann Arbor, MI, USA) (SIGIR â€™18).
ACM, New York, NY, USA, 515â€“524. https://doi.org/10.1145/3209978.3209991
[8] F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Trans. Interact. Intell. Syst. 5, 4, Article 19 (Dec. 2015), 19 pages.
https://doi.org/10.1145/2827872
[9] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural Collaborative Filtering. In Proceedings of the 26th International Conference on World Wide Web (Perth, Australia) (WWW â€™17). International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva,
Switzerland, 173â€“182. https://doi.org/10.1145/3038912.3052569
[10] Binbin Hu, Chuan Shi, Wayne Xin Zhao, and Philip S. Yu. 2018. Leveraging Metapath Based Context for Top- N Recommendation with A Neural Co-Attention Model. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (London, United Kingdom) (KDD â€™18). ACM, New York, NY, USA, 1531â€“1540. https://doi.org/10.1145/3219819.3219965
[11] Yifan Hu, Yehuda Koren, and Chris Volinsky. 2008. Collaborative Filtering for Implicit Feedback Datasets. In Proceedings of the 2008 Eighth IEEE International Conference on Data Mining (ICDM â€™08). 263â€“272.
[12] Walid Krichene, Nicolas Mayoraz, Steffen Rendle, Li Zhang, Xinyang Yi, Lichan Hong, Ed Chi, and John Anderson. 2019. Efficient Training on Very Large Corpora via Gramian Estimation. In International Conference on Learning Representations.
[13] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient Estimation of Word Representations in Vector Space. CoRR abs/1301.3781 (2013).
[14] Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. 2001. ItemBased Collaborative Filtering Recommendation Algorithms. In Proceedings of the 10th International Conference on World Wide Web (Hong Kong, Hong Kong)
(WWW â€™01). Association for Computing Machinery, New York, NY, USA, 285â€“295.
https://doi.org/10.1145/371920.372071
[15] Xiang Wang, Dingxian Wang, Canran Xu, Xiangnan He, Yixin Cao, and Tat-Seng Chua. 2019. Explainable Reasoning over Knowledge Graphs for Recommendation.
In Proceedings of the 33rd AAAI Conference on Artificial Intelligence (AAAI) (AAAI
â€™19). 5329â€“5336.
[16] Longqi Yang, Eugene Bagdasaryan, Joshua Gruenstein, Cheng-Kang Hsieh, and Deborah Estrin. 2018. OpenRec: A Modular Framework for Extensible and Adaptable Recommendation Algorithms. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining (Marina Del Rey, CA,
USA) (WSDM â€™18). ACM, New York, NY, USA, 664â€“672. https://doi.org/10.1145/
3159652.3159681
[17] Longqi Yang, Yin Cui, Yuan Xuan, Chenyang Wang, Serge Belongie, and Deborah Estrin. 2018. Unbiased Offline Recommender Evaluation for Missing-not-atrandom Implicit Feedback. In Proceedings of the 12th ACM Conference on Recommender Systems (Vancouver, British Columbia, Canada) (RecSys â€™18). ACM, New York, NY, USA, 279â€“287. https://doi.org/10.1145/3240323.3240355
[18] Hsiang-Fu Yu, Mikhail Bilenko, and Chih-Jen Lin. 2017. Selection of Negative Samples for One-class Matrix Factorization. In Proceedings of the 2017 SIAM International Conference on Data Mining. 363â€“371.

SUGGESTIONS

Our results have shown that a sampled metric can be a poor indicator of the true performance of recommender algorithms under this metric. For uncorrected metrics this is mostly due to the large bias introduced by sampling. Using correction methods, this bias can be reduced but at the cost of higher variance. If a study needs to use sampled metrics and is still interested in the true performance of the metrics, we suggest to use a correction method as proposed in this work. In this case it is important to rerun the experiment with different samples (e.g., different random seeds). It is already common, in most evaluations, to repeat an experiment ğ‘ times â€“
usually by varying the dataset (e.g., ğ‘ -fold cross validation). In this case, variance is introduced by the differences in the dataset split and potentially by the initialization of the recommender algorithm.
In a sampled evaluation, adding a different seed for negative sampling will add another source of variance. That means it may be harder to find â€œstatistically significant" differences between two recommenders. If even under the increased variance a difference is found, then this is a stronger indication that the recommender is truly better under the exact metric. The lower the bias in the corrected metric (e.g., the lower ğ›¾), the stronger the indication.
While this evaluation is preferable over uncorrected metrics, it is still prone to either not identifying differences (due to variance) or drawing false conclusions because of the bias. This bias can only be eliminated by avoiding sampling altogether.

8

CONCLUSION

This work seeks to bring attention to some issues with sampling of evaluation metrics. It has shown that most metrics are inconsistent under sampling and can lead to false discoveries. Moreover,
metrics are usually motivated by applications, e.g., does the top 10 list contain a relevant item? Sampled metrics do not measure the intended quantities â€“ not even in expectation. For this reason,
sampling should be avoided as much as possible during evaluation.
If an experimental study needs to sample, we propose correction methods that give a better estimate of the true metric, however at the cost of increased variance. Our analysis focused on the case of a single relevant item. The general case may be treated by making the approximation that observed ranks are independent, in which case similar correction methods can be applied. Deriving correction methods without independence is an interesting direction for future research.

1756

Research Track Paper

9

KDD '20, August 23â€“27, 2020, Virtual Event, USA

REPRODUCIBILITY

the following definition. The basic similarity is cosine and we follow the suggestion by [1] to apply an exponent ğ‘ to sharpen the similarity:
!ğ‘
|{ğ‘¢ : (ğ‘¢, ğ‘–) âˆˆ ğ» } âˆ© {ğ‘¢ : (ğ‘¢, ğ‘—) âˆˆ ğ» }|
ğ‘ ğ‘–,ğ‘— = p
(27)
p
|{ğ‘¢ : (ğ‘¢, ğ‘–) âˆˆ ğ» }| |{ğ‘¢ : (ğ‘¢, ğ‘—) âˆˆ ğ» }|

We provide further details about the evaluation in Section 6 to facilitate reproducibility. There are many different variations of matrix factorization and item-based collaborative filtering. In this section,
we clarify the exact model, loss function, and hyper-parameters that we used.
Let ğ‘ˆ be the set of users, ğ¼ be the set of items, and ğ» âŠ† ğ‘ˆ Ã— ğ¼ be the training set of items that the user has rated in the past.
Recommender X is a matrix factorization model, for which the output for a pair (ğ‘¢, ğ‘–) âˆˆ ğ‘ˆ Ã— ğ¼ is given by
ğ‘¦Ë† (ğ‘¢, ğ‘–) =

ğ‘‘
Ã•

Then we add the option to sparsify based on k-nearest neighbors:
â€²
ğ‘ ğ‘–,ğ‘—
= ğ‘ ğ‘–,ğ‘— ğ›¿ (ğ‘– âˆˆ ğ‘ğ‘˜ ( ğ‘—)) ğ›¿ ( ğ‘— âˆˆ ğ‘ğ‘˜ â€² (ğ‘–)),

where ğ‘ğ‘˜ (ğ‘–) are the ğ‘˜ closest items to ğ‘– based on ğ‘ . We allow both selecting for row or column neighbors. Then we apply row normalization:
â€²
ğ‘ ğ‘–,ğ‘—
â€²â€²
(29)
ğ‘ ğ‘–,ğ‘—
= â€² .
||sğ‘– ||

ğ‘£ğ‘¢,ğ‘“ ğ‘£ğ‘–,ğ‘“ ,

ğ‘“ =1

where ğ‘£ğ‘¢,ğ‘“ , ğ‘£ğ‘–,ğ‘“ are the parameters of the model. We use the parameterization of the loss as proposed in [3]:
Ã•
Ã•Ã•
argmin
(ğ‘¦Ë† (ğ‘¢, ğ‘–) âˆ’ 1) 2 + ğ›¼
ğ‘¦Ë† (ğ‘¢, ğ‘–) 2 + ğœ†||ğ‘‰ || 2ğ¹ ,
ğ‘‰

(ğ‘¢,ğ‘–) âˆˆğ»

(28)

Finally the prediction is:
Ã
ğ‘¦Ë† (ğ‘¢, ğ‘–) =

ğ‘¢ âˆˆğ‘ˆ ğ‘– âˆˆğ¼

optimized with implicit alternating least squares [11]. The hyperparameters are ğ‘‘ = 16, ğœ† = 10, ğ›¼ = 0.2.
Recommenders Y and Z are item based collaborative filtering algorithms [14]. There are many different variations of this algorithm, in particular how to generate the similarity matrix. We use

â€²â€²
ğ‘—:(ğ‘¢,ğ‘—) âˆˆğ» ğ‘ ğ‘–,ğ‘—
Ã
.
â€²â€²
ğ‘— âˆˆğ¼ ğ‘ ğ‘–,ğ‘—

(30)

Recommender Y uses the following hyperparameters: ğ‘ = 3, ğ‘˜ =
âˆ, ğ‘˜ â€² = âˆ. Recommender Z uses the following hyperparameters:
ğ‘ = 1, ğ‘˜ = âˆ, ğ‘˜ â€² = 10.

1757


