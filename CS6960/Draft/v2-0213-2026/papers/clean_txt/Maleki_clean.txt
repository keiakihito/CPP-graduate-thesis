Expert Systems With Applications 255 (2024) 124473

Contents lists available at ScienceDirect

Expert Systems With Applications journal homepage: www.elsevier.com/locate/eswa

A music recommender system based on compact convolutional transformers Negar Pourmoazemi âˆ—, Sepehr Maleki NewDay Ltd, 7 Handyside Street London, UK

ARTICLE

INFO

Keywords:
Music recommender systems Genre classification Convolutional neural networks Transformers Compact convolutional transformer

ABSTRACT In recent years, music streaming services have garnered a surge in popularity. Maintaining a continuous flow of music that aligns with user preferences, commonly known as the continuity problem, has become a significant issue in this domain. To address this challenge, developing Music Recommender Systems (MRSs) that can automatically search through vast music libraries and suggest appropriate songs to listeners is crucial. To this end, this paper proposes a Compact Convolutional Transformer (CCT) model for improving the feature selection process and thus addressing the continuity problem based on music genres. The model extracts latent features from Mel-spectrograms generated from raw audio songs. Then, the cosine similarity measure determines the similarity between feature maps to recommend the most relevant songs. Several methodologies, including two state-of-the-art CRNN models, are used to benchmark the modelâ€™s performance. The experimental results demonstrate that the proposed model significantly outperforms the current state-of-the-art models in terms of precision, recall, F1 score, and overall accuracy while having significantly fewer parameters.

1. Introduction Listening to music is a widespread leisure activity in the contemporary society. The music industry has undergone a digital transformation, and there has been a significant increase in the consumption and the sale of digital records. Consumers have shifted from purchasing physical copies to acquiring audio files and using music streaming platforms. According to Statista, the number of Spotify Premium Subscribers has increased from 18M users to 210M between 2015 and 2023, while this number for Apple Music has increased from 6.5M in 2015 to 78M in 2021. Retaining such user-bases requires continuous improvement of the methodologies that address the information overload for users and the playlist continuity problem. MRSs are designed to address this challenge by learning usersâ€™ taste and behaviour and suggest relevant songs based on individual preferences. These systems benefit music consumers and vendors by facilitating music discovery and enhancing sales.
MRSs generate personalized recommendations by leveraging explicit (e.g., user ratings) and implicit (e.g., number of times a song is played) information. Developing an MRS solely based on implicit information is a non-trivial task. The information is extracted from raw audio signals or metadata (such as artist name, date of publication,
and various tags). Extracting features from audio signals is particularly challenging due to the large semantic gap between the musical characteristics and the corresponding audio signals. While some characteristics, such as the existence of a particular instrument, are relatively

easy to extract, others, such as genre-based features, are more difficult due to several relevant indicators in the audio signal, such as rhythmic structure and harmonic content. Furthermore, genre classification is an ambiguous process since the genre needs to be a well-defined concept (Tzanetakis & Cook, 2002). This challenge has led to the emergence of genre-based music recommendation systems, which have gained considerable attention in Music Information Retrieval (MIR).
1.1. Audio features The most popular implicit features for genre-based MRSs are spectrograms (Elbir & Aydin, 2020). They contain genre-related features such as pitch, rhyme, and harmony patterns. A spectrogram is obtained by applying the Short-Time Fourier Transform (STFT) of overlapping windows on the audio signal. This process converts the decomposition of each window into frequencies. Each output window constitutes a frame, and spectrograms are generated by concatenating successive frames. In the generated frame, the ğ‘¥-axis and ğ‘¦-axis represent time and frequency, respectively. Moreover, colours represent amplitude in various frequencies. Fig. 1(a) shows a spectrogram of an audio file.
One issue with the spectrogram is that, as can be seen in Fig. 1(a),
only a very small portion at the lower part of the spectrogram is visible. Another issue with spectrograms is that they use Hz, which is problematic considering humans perceive sounds logarithmically.
For example, the difference for both 500 Hz âˆ’1000 Hz and 7500 Hzâ€“
8000 Hz ranges is 500 Hz. However, since humans mostly hear sounds

âˆ— Corresponding author.

E-mail addresses: negar.pourmoazemi@newday.co.uk (N. Pourmoazemi), smaleki@lincoln.ac.uk (S. Maleki).
https://doi.org/10.1016/j.eswa.2024.124473 Received 1 July 2023; Received in revised form 19 April 2024; Accepted 7 June 2024 Available online 12 June 2024 0957-4174/Â© 2024 Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies.

Expert Systems With Applications 255 (2024) 124473

N. Pourmoazemi and S. Maleki

Fig. 1. Comparison of the spectrogram of an audio signal with its Mel-spectrogram.

that are concentrated in small frequency and amplitude ranges, they can only notice the difference in the former range. Therefore, a small spectrogram adjustment is needed to convert frequencies (y-axis) into a log scale and the amplitudes to Decibels, the log scale of amplitudes.
Therefore, after applying the STFT on audio signals, the frequency and amplitude values are converted to a logarithmic scale, called Mel scale,
to create Mel-spectrograms (Dieleman & Schrauwen, 2014). Fig. 1(b)
shows mel-spectrogram of the same audio file that its spectrogram is shown in Fig. 1(a). Mel-spectrograms generally perform better regarding the Area Under the ROC (Receiver Operating Characteristic) curve abbreviated as AUC (Choi, Fazekas, & Sandler, 2016).

and input to the model. The results showed CNNâ€“LSTM achieved a better accuracy. Russo et al. introduced (Russo, KraljeviÄ‡, Stella, &
Sikora, 2020) a cochleogram-based system for detecting the affective musical content that closely matches the characteristics of human hearing. This proposed approach uses a CNN architecture to extract the music features from the cochleogram images. Dang et al.
investigated (Dang, Moreno-GarcÃ­a, & Prieta, 2021) sentiment analysis in MRSs using two CNN-based architectures with collaborative filtering. They demonstrated that their hybrid approach significantly improves the performance of the recommender system. Bahuleyan et al. proposed (Bahuleyan, 2018) a transfer learning method using a CNN-based feature extractor combined with XGBoost to enhance the performance of a vanilla CNN-based feature extractor. The proposed model outperforms the vanilla CNN as XGBoost is an ensembling method that combines multiple decision trees to obtain a better model in terms of performance and speed. Gunawan et al. introduced (Gunawan, Suhartono, et al., 2019), a CRNN architecture showing slightly higher accuracy than CNN-based methods.

1.2. Review of automated feature extraction methods The traditional approach to feature extraction in audio signals involves manual annotation by experts, which is a time-consuming and labour-intensive process requiring specialized knowledge of audio signals. In recent years, automated techniques have been proposed to accelerate this process. One such example is the FPGA-based feature extractor for musical genre classification proposed by Wassi, Iloga, Romain, Granado, and TchuentÃ© (2018), which enables automatic audio indexing of broadcast data from the standard FM radio band. Another example is the Hybrid Music Recommender System (HMRS) proposed by Katarya and Verma (2018), which extracts the properties of music from the userâ€™s listening history and recommends music based on the userâ€™s contextual preferences. Saari et al. (2015) propose a genreadaptive and semantic computing modelling technique for automatic music mood classification, demonstrating that taking the genre into account is beneficial for automatic music mood annotation. Ndou,
Ajoodha, and Jadhav (2021) used the Information Gain Ranking algorithm to select the most contributing features for a genre classification problem. They then investigated the performance of several machine learning models, such as KNN, Random Forest, and SVM. The results showed that KNN performed better in terms of accuracy.
Given the effectiveness of Deep Learning (DL) techniques in feature extraction in various domains such as image processing (e.g.,
(Saxena, Shukla, & Gyanchandani, 2020)), natural language processing (e.g., (Javaloy & GarcÃ­a-Mateos, 2020)), and video processing
(e.g., (Hu, Turki, Phan, & Wang, 2018)), DL has been increasingly applied in MRSs to extract and leverage latent features. DL can automatically extract features by constructing models representing data at different abstraction levels (Zhang, Yao, Sun & Tay, 2019). Convolutional Neural Networks (CNNs) have become a famous DL architecture for feature extraction. Cheng, Chang, and Kuo (2020) used CNNs in a genre-classification problem. They generated Mel-spectrograms from audio data and fed them the model. Srivastava, Ruhil, and Kaushal
(2022) proposed two CRNN algorithms for music genre classification:
CNN-GRU and CNNâ€“LSTM. They used MFCCs to represent audio data

1.3. Challenges of the domain and our contribution Despite the growing interest in DL-based approaches to MRSs, a few challenges exist. These are:
(i) Feature extraction for genre pattern identification (Lops, Jannach, Musto, Bogers, & Koolen, 2019).
(ii) Lack of transparency since the latent features do not explain why a specific item is recommended (Zhang, Yang, Ma, & Wu, 2019).
(iii) Lack of established datasets with sufficient required metadata
(Lops et al., 2019).
(iv) Various available evaluation metrics (Kunaver & PoÅ¾rl, 2017).
(v) Reflecting user intentions into sequential mod is a challenging task (Zhang, Li, Wang, Hossain, & Lu, 2020). For example, in MRSs, recommending a song based on the songs the user has clicked on can be challenging as the user may accidentally click some items.
(vi) Difficulty dealing with the same trackâ€™s variants (Lops et al.,
2019). For example, a song with the same lyrics can be available in different genres.
This paper aims to address the challenge of feature extraction and dealing with the same trackâ€™s variants. The main contribution of this paper is introducing a novel CCT-based model to address this challenge.
The proposed model employs convolutional layers to extract latent features from mel-spectrograms. These latent features are then reshaped to form a sequence and passed to a transformer block. The transformer block employs a self-attention mechanism to capture interdependencies between different positions in the sequence by varying attention weights to each element. This enables the model to focus on the most 2

Expert Systems With Applications 255 (2024) 124473

N. Pourmoazemi and S. Maleki Table 1 Summary of the GTZAN dataset.
Number of audio tracks Length of each audio track Genres Audio format Sample rate

dataset can result in suboptimal generalization performance. Therefore, each song is divided into 21 overlapping segments, resulting in a dataset of 21,000 instances better suited for training. Finally, we generate mel-spectrograms from the audio clips, which serve as input to our proposed model. Fig. 2 shows the mel-spectrograms generated for different genres. It can be seen that each genre exhibits distinguishable characteristics. Therefore, considering the appropriate features, the recommender system can determine which genre to recommend.

1,000 30 s 10 WAV 22,050 Hz

Table 2 CNN block parameters.
Parameter name

Parameter value

Kernel size Number of filters â€” 1st and 2nd layer Number of filters â€” 3rd and 4th layer Number of filters â€” 5th and 6th layer CNN stride Padding Activation function Pool size Pool stride

(3, 3)
32 64 128 1 same ReLU
(2, 2)
(2, 2)

2.1. Feature extraction and genre classification Feature extraction is one of the critical components of an MRS. We propose a CCT model as the feature extractor, which integrates a CNN and a transformer block. Fig. 3 shows a diagram of the proposed model.
The mel-spectrograms are fed to the model as the input. The first part of the model is a convolutional block composed of six convolution layers and a pooling layer after every two convolutions. The convolution layers use the Rectified Linear Unit(ReLU) activation function. In the ReLU activation function, only a few neurons are activated at a time, making the network sparse. Hence, ReLU is less computationally expensive than other activation functions such as Tanh and Sigmoid. The parameters of the CNN block are presented in Table 2. The output of the CNN block is reshaped into sequences, which are then transferred to a positional embedding layer to denote the relative position of each entity in a sequence. The positional embeddings are then fed to the transformer block.
The transformer block includes an encoder and a sequence pooling layer. Using its attention mechanism, the encoder helps the model focus on essential elements of the transformer input. The encoder in the proposed model comprises a stack of two identical layers, each with two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, fully connected feed-forward layer. Moreover,
Layer normalization is used in the encoder to normalize the output of each layer. Fig. 4 shows the encoder structure.
The multi-head self-attention mechanism in each sub-layer consists of two parallel attention functions. It is beneficial to linearly project the queries, keys, and values â„ times with different learned attention values (Vaswani et al., 2017). An attention function can be described as mapping a query and a set of keyâ€“value pairs to an output. The output is computed as a weighted sum of the values, where a compatibility function of the query with the corresponding key computes the weight assigned to each value. Fig. 5 represents the structure of an attention function. Two parallel attention functions are then concatenated and projected using a linear feed-forward layer, resulting in the final output with important elements focused on. Fig. 6 represents multi-head attention consisting of two attention layers running in parallel.
The encoder output is then sent to a sequence pooling layer that weights the encodersâ€™ output. Table 3 represents the transformerâ€™s parameters. The ReLU activation function is chosen for the encoderâ€™s feed-forward layer as it is computationally less expensive since it does not activate all neurons at the same time.
Finally, the output of the sequence pooling layer is passed to a classifier. The classifier is a fully-connected layer with 10 units and a Softmax activation function. The Softmax activation function is chosen as it is used for multi-class classification problems. It maps inputs to a probability distribution over multiple classes. The classifierâ€™s output in the proposed model is the predicted probabilities of each genre, and the genre with the highest probability gets picked as the model output.
We divided the dataset into training, validation and testing subsets with proportions of 0.7, 0.2 and 0.1, respectively. Moreover, stratified sampling is used when splitting the dataset into subsets so the percentage of each genre is equal across all subsets. For training, we used categorical cross-entropy as the loss function and Adam optimizer with weight decay of 0.001 and learning rate of 0.0001. The model was trained on the training and validation sets for 250 epochs.

relevant information. Consequently, the Transformer can capture longrange and global interdependencies and learn intricate relationships between feature map sequences (Vaswani et al., 2017) and identify regions in the feature maps that are more important for predicting the music genre and future recommendations. Owing to its transformerbased architecture that can simultaneously attend to different parts of the audio signal and learn how they are related, our proposed model achieves state-of-the-art recommendation performance compared to alternatives such as the model proposed by Cheng et al. (2020) and Srivastava et al. (2022). Notably, our model has significantly fewer trainable parameters, which is particularly important for faster training and prediction in recommender systems where continuity is crucial.
The proposed approach offers a reliable and effective mean for suggesting songs to users with similar musical preferences. For the validation of our study, we used the GTZAN dataset (George, Georg, &
Perry, 2001), owing to its well-established position as a benchmark for music genre classification tasks (Cheng et al., 2020; Ndou et al., 2021;
Srivastava et al., 2022). Initially introduced for genre classification of songs, the GTZAN dataset comprises 1000 30-s audio snippets in the format of â€˜â€˜.wavâ€™â€™ files. Each audio file belongs to one of the ten distinct genres: Hip-Hop, Rock, Reggae, Classical, Jazz, Blues, Pop,
Disco, Country, and Metal. The dataset is balanced such that it consists of 100 audio files per genre with labels assigned to their corresponding categories. Table 1 summarizes the properties of the GTZAN dataset.
2. Methodology Audio signals are generated from low and high air pressure variations, and the intensity of these variations is referred to as amplitude,
which can be measured and plotted over time. To process audio signals,
they must be quantized in both time and amplitude. This quantization process involves sampling at regular time intervals and measuring the amplitude of each sample. The sampling rate, the number of samples per second, is the sole parameter of the sampling process. Generally, a higher sampling rate provides a more accurate representation of the audio signal, while a lower sampling rate results in missing a considerable amount of information. In audio signal processing, a common sampling rate is 22,050 Hz (Wyse, 2017), which equates to 22,050 samples per second for a 1-s audio clip. Each song in the GTZAN dataset is 30 s long,
so a sampling rate of 22,050 Hz generates 661,500 samples. However,
some of the songs in the dataset subceed 30 s in duration. Therefore,
we trim all songs only to consider the first 660,000 samples to prevent any size mismatches.
Despite the widespread use of the GTZAN dataset for music genre classification tasks, the limited number of training examples in the 3

Expert Systems With Applications 255 (2024) 124473

N. Pourmoazemi and S. Maleki

Fig. 2. Mel-spectrograms of different genres.

Fig. 3. CCT architecture.

Fig. 4. Encoder architecture.

Table 3 Encoder parameters.
Parameter name

Parameter value

Epsilon in layer normalization Number of attention heads Size of attention head for query and key Attention head dropout probability Feed forward units Feed forward activation function

0.00001 2 128 0.1 128 ReLU

Fig. 5. Attention mechanism.

4

Expert Systems With Applications 255 (2024) 124473

N. Pourmoazemi and S. Maleki

Table 4 Accuracy values.
Set

Accuracy

Training Validation Test

0.9857 0.9444 0.9375

Table 5 Evaluation metrics for the proposed model.
Metric

Precision

Recall

F1-score

Value

0.923

0.928

0.923

training and validation curves. Furthermore, the test set evaluates the model using the test accuracy metric.
Table 4 represents the accuracy values for the proposed modelâ€™s training, validation, and test sets.
The normalized confusion matrix is also given Fig. 8.
Furthermore, for benchmarking, ğ…ğŸ score, Precision, and Recall are used as three metrics to evaluate the performance of our model:
ğ‘ƒ ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› =
Fig. 6. Multi-head attention mechanism.

ğ‘‡ğ‘
ğ‘‡ğ‘ + ğ¹ğ‘

,

ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ =

ğ‘‡ğ‘
ğ‘‡ğ‘ + ğ¹ğ‘›

,

ğ¹1 = 2 Ã—

ğ‘ƒ ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› . ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™
,
ğ‘ƒ ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› + ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™

where ğ‘‡ğ‘ , ğ¹ğ‘ , and ğ¹ğ‘› are the number of true positives, false positives,
and false negatives, respectively. Table 5 shows these metrics. Precision is 0.923, meaning that out of all songs predicted as a genre, 92.3% are in that genre and predicted correctly. Recall is 0.928, meaning that out of all samples in each genre, 92.8% of them are predicted correctly.
Finally, the F1-score is 0.923, showing 92.3% of the time the model makes a correct prediction across the entire dataset.
Finally, Table 6 shows the prediction probabilities of the new songsâ€™
genres. It can be seen that the model correctly predicts the genres with a high probability. In addition to the out-performance of the model, it only includes 454,187 parameters. The fewer parameters in the model result in significantly less computational complexity and, therefore, less training time.

2.2. Recommendation Cosine similarity was used for recommending similar songs. The cosine similarity metric is defined as the cosine of the angle between two n-dimensional vectors (in this case, feature maps) and is calculated using Eq. (1):
âˆ‘ğ‘›
ğ‘âƒ— . ğ‘âƒ—
ğ‘–=1 ğ‘ğ‘– Ã— ğ‘ğ‘–
âƒ— =
.
(1)
= âˆš
ğ‘ ğ‘–ğ‘šğ‘–ğ‘™ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘¦ ( âƒ—
ğ‘, ğ‘)
âˆšâˆ‘
âˆ‘
âƒ—
ğ‘›
ğ‘›
2 2
â€–âƒ—
ğ‘â€– Ã— â€–ğ‘â€–
ğ‘–=1 ğ‘ğ‘– Ã—
ğ‘–=1 ğ‘ğ‘–
where ğ‘âƒ— and ğ‘âƒ— are vectors whose cosine similarity needs to be calculated. The cosine similarity measure performs more efficiently than traditional machine learning algorithms such as k-nearest neighbour (Jaiswal, Kharade, Kotambe, & Shinde, 2020; Sarwar, Karypis,
Konstan, & Riedl, 2001) and Support vector Machines (Al-Anzi &
AbuZeina, 2017). Among other similarity metrics, cosine similarity and Jaccard coefficient are less complex and thus less computationally expensive in calculating distances (Sondur, Chigadani, & Nayak, 2016).
However, the data size highly influences the Jaccard coefficient, while cosine similarity performs better in high dimensional space (Zahrotun,
2016). Therefore, the cosine similarity measure is better suited for genre classification where large datasets are required.
Feature maps are obtained from the last layer before the classifier.
Therefore, the classifier is removed from the trained model to obtain the feature maps of the songs in the dataset. Then, a song for each genre, apart from those in the dataset, is downloaded, their melspectrograms are generated, and their feature maps are obtained using the trained model. Then, the recommendation is made for each song by calculating the cosine similarity between its feature maps and the songs in the dataset. It can be seen the genre of recommended songs is also the same as the obtained genres for the new song. Furthermore,
the genre of these new songs is calculated using the model (with the classifier) to check whether the model can correctly predict the genres.

3.1. Benchmarking and comparison We have compared the performance of our model with several techniques in the literature using the benchmarking metrics Precision, Recall, F1-Score, and Accuracy and the GTZAN dataset. The techniques can be broadly classified into classical approaches, including KNN-based (Karunakaran & Arya, 2018; Ndou et al., 2021), LinearSVM (Karunakaran & Arya, 2018), PolySVM (Karunakaran & Arya,
2018), and deep learning-based methods that include CNNs (Elbir,
Ã‡am, Iyican, Ã–ztÃ¼rk, & Aydin, 2018), GRUCNN (Srivastava et al.,
2022), LSTMCNN (Srivastava et al., 2022) (the state-of-the-art), Multimodal (Jena, Bhoi, Mohapatra, & Bakshi, 2023), and Hybrid (Jena et al., 2023). The result of this comparison is reported in Table 7.
It can be seen that the test accuracy of the proposed CCT model is higher than that of the state-of-the-art CRNN model (Srivastava et al.,
2022), which uses LSTM blocks. Considering model complexities and computational costs, the classical approaches perform reasonably well.
Comparing the deep learning-based models, it can be seen that those models that consider the sequential dependencies of the data perform better. These include GRUCNN, LSTMCNN and our proposed CCT-based model. The Multimodal approach, proposed in various literature (Jena et al., 2023; Oramas, Nieto, Barbieri, & Serra, 2017) uses a combination of wavelet generation and spectrogram analysis, which does not improve previously reported results. However, when combined with transfer learning as proposed in Jena et al. (2023), a hybrid model shows a significant performance improvement.
CNN-based models are well-suited for extracting local features from audio signals, and transformers are well-suited to capture long-range dependencies. Transformers can outperform RNN architectures such

3. Experimental results We have tested the efficacy of our model on the GTZAN dataset and compared its performance with recent techniques, including the stateof-the-art. Fig. 7 shows the performance of the training and validation sets during the training process. It can be seen that the model converges without overfitting, as there is only a negligible gap between the 5

Expert Systems With Applications 255 (2024) 124473

N. Pourmoazemi and S. Maleki

Fig. 7. Training and evaluation performance.

Fig. 8. Normalized confusion matrix of test set.

4. Conclusions

as GRU-based and LSTM-based as they can simultaneously attend to different parts of the audio signal and learn how they are related. Therefore, a CCT-based model can outperform the state-of-the-art CRNN models. Furthermore, the compact design of CCTs makes them suitable for resource-constrained environments, such as mobile devices. This is because CCTs avoid using large, fully-connected, computationally expensive layers.

A CCT model was developed to address genre classificationâ€™s feature extraction challenge. For benchmarking, the proposed model was compared to the recent state-of-the-art CRNN model (Srivastava et al.,
2022). It was shown that our model produces better precision, recall,
F1-score, and accuracy results.
6

Expert Systems With Applications 255 (2024) 124473

N. Pourmoazemi and S. Maleki Table 6 Predicted probability of each genre for different songs.
Song name

Ground truth

Recode Be good 2 me Song for Sienna Where you come from Autumn in New York Country again Rain on me Dear America go down deh Making a fire

Metal Disco Classical Hip hop Jazz Country Pop Blues Reggae Rock

Metal 99.999 0.080 0.014 0.000 0.000 0.000 0.000 0.000 0.000 0.108

Disco 0.000 98.791 0.001 0.000 0.001 0.000 0.000 0.000 0.000 0.000

Predicted probability for each genre Hip hop Jazz Country Pop 0.000 0.000 0.000 0.000 0.023 0.030 0.016 0.114 0.001 0.011 0.001 0.014 100.000 0.000 0.000 0.000 0.000 99.998 0.000 0.000 0.000 0.000 99.999 0.001 0.002 0.000 0.000 99.998 0.013 0.738 0.000 0.000 0.001 0.000 0.000 1.633 0.029 0.001 0.002 0.016

Classical 0.000 0.010 99.956 0.000 0.002 0.000 0.000 0.000 0.000 0.001

Blues 0.000 0.022 0.000 0.000 0.007 0.000 0.000 99.249 0.000 0.004

Reggae 0.000 0.657 0.001 0.000 0.000 0.000 0.000 0.000 98.365 0.039

Rock 0.001 0.258 0.001 0.000 0.000 0.000 0.000 0.000 0.000 99.800

Table 7 Comparison of performance metrics on the GTZAN dataset.
Method

Precision

Recall

F1-Score

Accuracy

Classical

KNN (Karunakaran & Arya, 2018)
LinearSVM (Karunakaran & Arya, 2018)
PolySVM (Karunakaran & Arya, 2018)

0.70 0.68 0.79

0.64 0.60 0.78

0.66 0.63 0.78

0.64 0.60 0.77

Deep Learning-based

CNN (Elbir et al., 2018)
GRUCNN (Srivastava et al., 2022)
LSTMCNN (Srivastava et al., 2022)
Multimodal (Jena et al., 2023)
Hybrid (Jena et al., 2023)
CCT

0.69 0.86 0.87 0.59 0.80 0.92

0.65 0.86 0.86 0.58 0.80 0.93

0.65 0.85 0.88 0.58 0.80 0.92

0.66 0.85 0.87 0.58 0.81 0.94

Furthermore, the recommendation for a new song of each genre is made. It was shown that the model could predict the genres with a high probability, which indicates that the music genre recommendation is reliable. Another MRS challenge answered in this thesis is dealing with variants of the same track. Since this paper deals with genrebased MRS, this problem is solved. Furthermore, the feature extraction problem is also solved as the last layer before the classifier produces the feature maps for each song.
We have shown that CCTs are effective for music genre classification. However, there are some potential limitations to their use. One limitation is that the CCT only takes spectrograms as input since the first block of the model is CNN. So, the song metadata cannot be used directly for capturing feature maps and making recommendations.
Additionally, CCTs can be sensitive to the choice of hyperparameters.
This means that the performance of the CCT can vary depending on the values of the hyperparameters, such as the number of layers, the number of neurons per layer, and the learning rate.

Bahuleyan, H. (2018). Music genre classification using machine learning techniques.
arXiv preprint arXiv:1804.01149.
Cheng, Y.-H., Chang, P.-C., & Kuo, C.-N. (2020). Convolutional neural networks approach for music genre classification. In 2020 international symposium on computer,
consumer and control (pp. 399â€“403). IEEE.
Choi, K., Fazekas, G., & Sandler, M. (2016). Automatic tagging using deep convolutional neural networks. arXiv preprint arXiv:1606.00298.
Dang, C. N., Moreno-GarcÃ­a, M. N., & Prieta, F. D. l. (2021). An approach to integrating sentiment analysis into recommender systems. Sensors, 21(16), 5666.
Dieleman, S., & Schrauwen, B. (2014). End-to-end learning for music audio. In 2014 IEEE international conference on acoustics, speech and signal processing (pp.
6964â€“6968). IEEE.
Elbir, A., & Aydin, N. (2020). Music genre classification and music recommendation by using deep learning. Electronics Letters, 56(12), 627â€“629.
Elbir, A., Ã‡am, H. B., Iyican, M. E., Ã–ztÃ¼rk, B., & Aydin, N. (2018). Music genre classification and recommendation by using machine learning techniques. In 2018 innovations in intelligent systems and applications conference (pp. 1â€“5). IEEE.
George, T., Georg, E., & Perry, C. (2001). Automatic musical genre classification of audio signals. In Proceedings of the 2nd international symposium on music information retrieval, Indiana: vol. 144.
Gunawan, A. A., Suhartono, D., et al. (2019). Music recommender system based on genre using convolutional recurrent neural networks. Procedia Computer Science,
157, 99â€“109.
Hu, Z., Turki, T., Phan, N., & Wang, J. T. (2018). A 3D atrous convolutional long short-term memory network for background subtraction. IEEE Access, 6,
43450â€“43459.
Jaiswal, S., Kharade, T., Kotambe, N., & Shinde, S. (2020). Collaborative recommendation system for agriculture sector. In ITM web of conferences: vol. 32, (p. 03034).
EDP Sciences.
Javaloy, A., & GarcÃ­a-Mateos, G. (2020). Preliminary results on different text processing tasks using encoder-decoder networks and the causal feature extractor. Applied Sciences, 10(17), 5772.
Jena, K. K., Bhoi, S. K., Mohapatra, S., & Bakshi, S. (2023). A hybrid deep learning approach for classification of music genres using wavelet and spectrogram analysis.
Neural Computing and Applications, 1â€“26.
Karunakaran, N., & Arya, A. (2018). A scalable hybrid classifier for music genre classification using machine learning concepts and spark. In 2018 international conference on intelligent autonomous systems (pp. 128â€“135). IEEE.
Katarya, R., & Verma, O. P. (2018). Efficient music recommender system using context graph and particle swarm. Multimedia Tools and Applications, 77(2), 2673â€“2687.
Kunaver, M., & PoÅ¾rl, T. (2017). Diversity in recommender systemsâ€“A survey.
Knowledge-Based Systems, 123, 154â€“162.
Lops, P., Jannach, D., Musto, C., Bogers, T., & Koolen, M. (2019). Trends in content-based recommendation. User Modeling and User-Adapted Interaction, 29(2),
239â€“249.
Ndou, N., Ajoodha, R., & Jadhav, A. (2021). Music genre classification: A review of deep-learning and traditional machine-learning approaches. In 2021 IEEE international IOT, electronics and mechatronics conference (pp. 1â€“6). IEEE.

CRediT authorship contribution statement Negar Pourmoazemi: Conceptualization, Investigation, Methodology, Software, Formal analysis, Writing â€“ original draft, Writing â€“
review & editing. Sepehr Maleki: Conceptualization, Writing â€“ original draft, Writing â€“ review & editing.
Declaration of competing interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.
Data availability Data will be made available on request.
References Al-Anzi, F. S., & AbuZeina, D. (2017). Toward an enhanced Arabic text classification using cosine similarity and Latent Semantic Indexing. Journal of King Saud University-Computer and Information Sciences, 29(2), 189â€“195.
7

Expert Systems With Applications 255 (2024) 124473

N. Pourmoazemi and S. Maleki

Tzanetakis, G., & Cook, P. (2002). Musical genre classification of audio signals. IEEE Transactions on Speech and Audio Processing, 10(5), 293â€“302.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.
(2017). Attention is all you need. In Advances in neural information processing systems
(pp. 5998â€“6008).
Wassi, G., Iloga, S., Romain, O., Granado, B., & TchuentÃ©, M. (2018). FPGA-based simultaneous multichannel audio processor for musical genre indexing applications in broadcast band. Journal of Parallel and Distributed Computing, 119, 146â€“161.
Wyse, L. (2017). Audio spectrogram representations for processing with convolutional neural networks. arXiv preprint arXiv:1706.09559.
Zahrotun, L. (2016). Comparison jaccard similarity, cosine similarity and combined both of the data clustering with shared nearest neighbor method. Computer Engineering and Applications Journal, 5(1), 11â€“18.
Zhang, Y., Li, Y., Wang, R., Hossain, M. S., & Lu, H. (2020). Multi-aspect aware sessionbased recommendation for intelligent transportation services. IEEE Transactions on Intelligent Transportation Systems.
Zhang, Q., Yang, Y., Ma, H., & Wu, Y. N. (2019). Interpreting CNNS via decision trees.
In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
(pp. 6261â€“6270).
Zhang, S., Yao, L., Sun, A., & Tay, Y. (2019). Deep learning based recommender system:
A survey and new perspectives. ACM Computing Surveys, 52(1), 1â€“38.

Oramas, S., Nieto, O., Barbieri, F., & Serra, X. (2017). Multi-label music genre classification from audio, text, and images using deep features. arXiv preprint arXiv:1707.04916.
Russo, M., KraljeviÄ‡, L., Stella, M., & Sikora, M. (2020). Cochleogram-based approach for detecting perceived emotions in music. Information Processing & Management,
57(5), Article 102270.
Saari, P., Fazekas, G., Eerola, T., Barthet, M., Lartillot, O., & Sandler, M. (2015).
Genre-adaptive semantic computing and audio-based modelling for music mood annotation. IEEE Transactions on Affective Computing, 7(2), 122â€“135.
Sarwar, B., Karypis, G., Konstan, J., & Riedl, J. (2001). Item-based collaborative filtering recommendation algorithms. In Proceedings of the 10th international conference on world wide web (pp. 285â€“295).
Saxena, S., Shukla, S., & Gyanchandani, M. (2020). Pre-trained convolutional neural networks as feature extractors for diagnosis of breast cancer using histopathology.
International Journal of Imaging Systems and Technology, 30(3), 577â€“591.
Sondur, M. S. D., Chigadani, M. A. P., & Nayak, S. (2016). Similarity measures for recommender systems: a comparative study. Journal for Research, 2(3).
Srivastava, N., Ruhil, S., & Kaushal, G. (2022). Music genre classification using convolutional recurrent neural networks. In 2022 IEEE 6th conference on information and communication technology (pp. 1â€“5). IEEE.

8


