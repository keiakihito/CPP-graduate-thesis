\chapter{Background and Literature Review}

This chapter reviews prior work relevant to backend audio embeddings for music similarity and retrieval, with a particular focus on architectural model families, evaluation strategies, and the challenges posed by classical music archives. 
It supports the thesis focus on embedding-model capacity and its practical impact on retrieval performance in small-scale classical collections, while situating collaborative filtering, user modeling, and large-scale deployment considerations in a background role.

\section{Content-Based Recommendation and the Role of Audio Embeddings}

Music recommendation systems have traditionally been dominated by collaborative filtering approaches that leverage large-scale user interaction data. While effective for mainstream streaming platforms, these methods are less suitable for curated archives with sparse or nonexistent user feedback. In such settings, content-based recommendation methods, which rely on intrinsic properties of the audio signal, become essential.
Audio embeddings provide compact representations of musical content that can be used for similarity search, retrieval, and downstream recommendation tasks. Advances in deep learning have enabled the extraction of embeddings that capture timbral, harmonic, rhythmic, and temporal characteristics directly from audio signals.
Recent work highlights the effectiveness of pretrained audio representations in music information retrieval, while also identifying a gap in their systematic evaluation within recommender systems \cite{tamm2024}. 
This gap is particularly relevant for small, domain-specific collections such as classical music archives, where large-scale interaction data are unavailable and embedding behavior under constrained data regimes remains insufficiently studied.
Classical collections are often smaller, stylistically coherent, and rich in structural relationships between works, movements, and performances. However, these same properties raise questions about how well embedding models developed and evaluated on large, popular-music datasets transfer to the classical domain.


\section{Backend Audio Embedding Model Families}
These architectural distinctions reflect broader trends in audio representation learning, where advances in model design and pretraining strategies have reshaped the landscape of music information retrieval.
This thesis focuses on comparing backend audio embedding model families rather than individual end-to-end recommendation systems. 
Prior work in music information retrieval has explored a range of architectural approaches for learning audio representations. 
Comprehensive surveys categorize deep learning architectures for audio modeling into distinct families, including convolutional neural networks (CNNs), recurrent neural networks (RNNs), transformer-based models, and hybrid architectures \cite{zaman2023}. 

Historically, CNN and hybrid CNN–RNN models have dominated music-related tasks, particularly in supervised classification settings \cite{jiang2024}. 
More recently, transformer-based architectures have emerged as a prominent family for spectral audio representation, leveraging self-attention mechanisms to model long-range dependencies in audio signals \cite{zaman2023}.

\subsection{CNN-Based Models}
Compact CNN architectures have demonstrated efficient and high-performing solutions for music detection tasks \cite{reddy2022}.
CNN-based architectures are widely used for supervised music genre and tagging tasks, with performance depending on dataset scale and model design.
These characteristics make compact CNN architectures attractive for deployment in resource-constrained environments.
Due to their strong inductive biases, such as local connectivity and translation invariance, CNNs may offer advantages in small-data regimes, making compact architectures reasonable baselines in constrained archival settings.


\subsection{Hybrid CNN--RNN Models}
To better capture temporal structure in music, hybrid architectures combining CNN feature extractors with recurrent neural networks (RNNs) have been proposed. In these models, convolutional layers extract local features from spectrograms, while recurrent layers such as gated recurrent units (GRUs) or long short-term memory (LSTM) networks model longer-term temporal dependencies.
Hybrid CNN–RNN architectures integrate convolutional layers for spectral feature extraction with recurrent units to model temporal dependencies in music classification tasks \cite{lin2024}. 
These architectures combine local spectral feature extraction with sequence-level temporal modeling, providing a unified framework for capturing both spatial and temporal characteristics of music.

\subsection{CNN--Transformer and Transformer-Based Models}

CNN–Transformer hybrid architectures employ convolutional front-ends to extract local spectral features before passing them to transformer encoders for modeling long-range temporal dependencies \cite{pourmoazemi2024}. 
In contrast, pure transformer-based models such as the Audio Spectrogram Transformer (AST) operate directly on spectrogram representations using self-attention mechanisms to capture global context \cite{gong2021ast}. 

These approaches represent a contrasting architectural family to CNN and CNN--RNN models, often characterized by higher representational flexibility and reliance on large-scale data.

Recent transformer-based audio models increasingly rely on large-scale self-supervised pretraining to learn general-purpose representations from unlabeled audio corpora. 
For example, BEATs introduces an iterative self-supervised framework based on masked discrete label prediction using semantic acoustic tokenizers \cite{chen2023beats}. 
Such approaches demonstrate that carefully designed pretraining objectives and large-scale exposure can yield substantial improvements in downstream audio tasks, even without proportionally increasing model size.


However, these advantages are typically demonstrated on large and diverse datasets such as AudioSet. 
It remains unclear whether such scale-dependent benefits transfer to small, domain-constrained archival collections, where data diversity and pretraining conditions differ substantially.


\subsection{Compact and Efficient Models}
Across all architectural families, there is growing interest in compact and efficient models that reduce parameter count and inference cost while maintaining competitive performance. Such models are particularly relevant for archival systems and research prototypes, where computational resources may be limited.
Compact models are not a distinct architectural class but rather a design objective that can be applied to CNN-based, hybrid, or transformer-based approaches. Evaluating the trade-offs between efficiency and retrieval performance is therefore an important consideration when selecting backend embeddings for classical music archives.
In the context of this thesis, compact architectures represent the lower-capacity end of the experimental spectrum. Their parameter efficiency and reduced complexity make them particularly relevant when investigating whether increased model capacity yields meaningful gains in small-scale archival retrieval tasks.


\section{Evaluation Strategies for Music Embeddings}
The quality of audio embeddings is commonly assessed through downstream evaluation tasks. Prior work employs a variety of evaluation strategies, each with different implications for recommendation and retrieval.

\subsection{Classification-Based Evaluation}
Audio embeddings are often evaluated using classification-based benchmarks in music information retrieval \cite{gong2021ast, lin2024}. 
Metrics such as accuracy, precision, recall, and F1-score are commonly reported on benchmark datasets.
While classification performance provides insight into the discriminative power of embeddings, it does not directly measure their effectiveness for similarity search or ranking-based retrieval.
Moreover, classification benchmarks are typically conducted on large-scale, well-annotated datasets. In such settings, increased model capacity often translates into improved discriminative accuracy. However, it remains unclear whether gains observed in large-scale classification tasks meaningfully transfer to small-scale retrieval scenarios in which data diversity and label granularity are limited.
Furthermore, improvements in classification accuracy do not necessarily imply improved neighborhood structure in embedding space, which is critical for similarity-based retrieval tasks.

\subsection{Ranking-Based Evaluation for Retrieval}
Ranking-based metrics such as normalized discounted cumulative gain (NDCG), Precision@K, and Recall@K are standard evaluation measures in top-N recommendation and retrieval settings, as they explicitly model the positional importance of relevant items in ranked lists \cite{krichene2020}. 
These metrics emphasize the upper portion of the ranked list, reflecting realistic user browsing behavior in which users are unlikely to inspect items far down the list.
Furthermore, recent empirical evidence suggests that strong performance on classification benchmarks does not necessarily translate to superior retrieval effectiveness \cite{tamm2024}. 
This finding underscores the importance of evaluating embeddings directly within retrieval-oriented frameworks rather than relying solely on classification benchmarks.

However, recent work also demonstrates that ranking metrics are sensitive to evaluation design choices. Reduced or constrained candidate sets can decrease the discriminative power of ranking metrics and may even lead to contradictory system comparisons under certain conditions \cite{krichene2020, canamares2020}. 
In particular, smaller item pools increase the probability of ranking ties and reduce metric sensitivity to performance differences between models \cite{canamares2020}.
In inherently small archival collections, where the candidate pool itself is limited, similar effects may arise. Minor variations in embedding representations can therefore have disproportionate effects on measured ranking performance when the evaluation space is small. 
Accordingly, ranking metrics in constrained environments must be interpreted in relation to dataset scale, proxy granularity, and model capacity, as observed improvements may reflect limitations of evaluation sensitivity rather than genuine representational gains.

\subsection{Beyond Accuracy: Diversity and Long-Tail Considerations}
In addition to accuracy-oriented metrics, prior work has highlighted the importance of diversity and long-tail discovery in music recommendation.
Diversity-aware evaluation emphasizes the ability of a system to surface varied and less popular items, fostering exploration of unfamiliar music cultures \cite{porcaro2022}.
While diversity metrics are not the primary focus of this thesis, they provide useful context for interpreting ranking results in classical music archives, where exploration and discovery are often prioritized over popularity.
In small archival collections, diversity considerations may also interact with model capacity, as higher-capacity models might emphasize dominant stylistic patterns present in limited datasets, potentially affecting the diversity of retrieved items.


\section{User Modeling and System-Level Approaches (Background)}
Prior work models user behavior, preferences, and contextual signals to enhance personalization and recommendation accuracy \cite{lin2025}. 
In addition, sequential recommendation models leverage user interaction histories to generate personalized recommendations \cite{abbattista2024}. 
Context-aware approaches, such as geographically informed systems, incorporate additional contextual signals alongside interaction data to further refine personalization \cite{schedl2021}.
At the system level, production recommender systems must address engineering concerns such as scalability and latency. 
However, such considerations are beyond the scope of this thesis, which isolates and evaluates backend embedding quality independently of user modeling and large-scale deployment constraints.


\section{Synthesis and Research Gap}
The reviewed literature demonstrates significant progress in learning audio embeddings for music-related tasks, spanning CNN-based, hybrid, and transformer-based architectures.
Prior studies frequently associate increased architectural complexity and model capacity with improved performance on large-scale classification benchmarks and heterogeneous music datasets.


However, these conclusions are predominantly derived from settings with abundant training data and broad musical domains.
It remains unclear whether increased embedding-model capacity yields comparable benefits in small-scale, single-domain archives such as classical music collections.

In constrained settings, limited data diversity, small candidate sets, and coarse proxy definitions may reduce the practical impact of increased model complexity.
Despite the prominent role of embedding capacity in modern audio representation learning, there is limited empirical work examining its practical effectiveness under such small-scale archival conditions.

While evaluation protocols such as ranking-based metrics are widely adopted, prior work rarely investigates how embedding capacity interacts with proxy design and dataset scale in determining retrieval performance.
This interaction is particularly relevant for content-based recommendation systems in curated classical archives.

This thesis addresses this gap by systematically analyzing the influence of embedding-model capacity on similarity retrieval performance in a small-scale classical archive.
Through a hypothesis-driven experimental design, the study examines whether higher-capacity architectures provide measurable gains over more compact models and identifies conditions under which such gains emerge or diminish.

